---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('msmbstyle'))


options(htmltools.dir.version = FALSE)

library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")

```

# Overview {-}

Welcome to our lab manual for `Data Science for the Natural Environment (Fall 2021)` - it will be continuously updated throughout the semester. For the most part an electronic copy (on a laptop or tablet) is sufficient, at times you may be instructed to print a data or worksheet for lab.


## Course Format {-}

This is a lecture/lab course meeting three times a week (TR 11:10 – 12:25, W 13:30 – 15:55). Key concepts will be introduced in reading assignments, short ‘prelab’-type tutorials, or exercises that you will be expected to complete before class.

While lecture/lab time will incorporate brief refreshers and more in-depth demonstrations of some of the more complex topics, the focus will be on applying the concepts and ‘learning by doing’ by directly applying new skills to case studies and data sets from ecology, environmental science and public health


**BYOD: Bring your own device (to lab)**

You are required to bring your laptop for use during lab; a tablet will not be sufficient to participate though you are welcome to bring an additional device to have an extra screen to follow along an electronic version of the lab manual. Make sure to have a power cable and/or fully charged battery!


**Course Materials**

Readings will primarily assigned from [Wickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. " O'Reilly Media, Inc.".](https://r4ds.had.co.nz/index.html)


<br>



## Homework assignments {-}

You will spend a significant portion of lecture/lab time working on exercises and engaging with your peers and the instructor on an individual basis to troubleshoot, clarify misunderstandings, and discuss alternative approaches to problem solve.

Most of the exercises will be completed during lab/lecture time though occasionally you will be instructed to complete an exercise outside of. Once a module is complete you will have two days to review, synthesize, clean up/complete your code and answers and turn in the knitted document (`*.html`) through `Canvas`^[For example, if we wrap up a module on a Tuesday your homework will be due Thursday at 11:59pm]. No late or incomplete homework will be accepted or receive credit.

You are encouraged to work collaboratively both during/outside of class times but everyone should be submitting their own work. If you worked closely with a certain person please make a note at the beginning of your document.


### Due Sunday, Sept. 26th {-}

Complete [Chapter 13](https://bi449fa21.netlify.app/climate-change.html) in our lab manual. Now that you have gained additional `Rmarkdown skills`, use those to format your “report” that you will hand in. Code chunks should have comments, add answers to questions in plain text (not as code in chunks), use headers intentionally, and of course you have options like bullet points etc. as well. Make sure that your `html` file has knitted as expected before you upload it; this includes headers being correctly formatted, bullet points formatted etc. If it hasn't go back to the Rmarkdown file and fix them, re-knit and then upload.

When you are in the process of learning how to write up your results (and trying to keep them separate from your interpretation/discussion) it is perfectly fine to start with a simple structure that is formulaic and more akin to bulleted declarative sentences than carefully, crafted prose. Sometimes it helps you actually write them out as bullet points.
So an answer for Q13.24 could start like this ...

* We compared rates of increase in the last 200 years for global mean temperature and atmospheric CO2 concentrations to data from the last approx. 800,000 years.
* Over the last 200 years there is a significant relationship between time and temperature. The rate of temperature increase currently is ... C per year. For past time ranges rates of increase ranged from x - x (and then you could have subpoints with the time or a table for individual time ranges + rates).
* Statement about whether rate of increase is higher (steeper) than in the past
* Statement about current CO2 rate of increase + in the past.
* Statement about whether current CO2 and temperature rate of increase show the same trend.

Separating out individual components into bullet points helps you organize what components belong together. Once you get used to organizing your thoughts you will automatically start to transition into being able to write better descriptions of your results (regardless of whether tables or figures).

Good communication starts with good organization of your thoughts, if you know what you want to say and how everything fits together chances are much higher that somebody else will be able to follow what you are trying to communicate to them.


### Due Sunday, Sept. 19th {-}

Complete [Chapter 9](hhttps://bi449fa21.netlify.app/scientific-misconduct-and-the-reproducible-crisis.html) in our lab manual, no code chunks but be sure to read the `Rmarkdown` challenge and include that in your document. Submit your knitted html file through Canvas as HW09^[I've updated the HW assignment numbers to match the chapter numbers so we've skipped a few.].

Complete [Chapter 10](https://bi449fa21.netlify.app/responsible-research-is-reproducible-research.html) in our lab manual; no code chunks but be sure to read the `Rmarkdown` challenge and include that in your document. Submit your knitted html file through Canvas as HW10

Complete [Chapter 11](https://bi449fa21.netlify.app/project-management-revisited.html) in our lab manual; no code chunks but be sure to read the `Rmarkdown` challenge and include that in your document. Submit your knitted html file through Canvas as HW10



### Due Sunday, Sept. 12th {-}

Complete [Chapter 7](https://bi449fa21.netlify.app/welcome-to-the-tidyverse.html) in our lab manual, make sure your code is properly commented and all questions have been answered. Submit your knitted html file through Canvas as HW04.

Complete [Chapter 8](https://bi449fa21.netlify.app/catch-data-analysis.html) in our lab manual. Make sure your code is properly commented and you have answered all the questions. Most of your answers can be brief when describing results and drawing conclusions but still be sure to be comprehensive. Submit your knitted html file through Canvas as HW05.


### Due Friday, Sept. 3rd {-}

Complete [Chapter 5](https://bi449fa21.netlify.app/identifying-essential-fish-habitat-shark-nurseries-on-the-texas-coast.html) in our lab manual. Remember, most of the answers to the questions can be brief, and we are skipping Q4. Write your assignment in `Rmarkdown` and then knit it and submit the html file.

Complete [Chapter 6](https://bi449fa21.netlify.app/intro-to-data-frames.html) in our lab manual. This `Rmarkdown` file contains both plain text and code chunks. It will only knit of all of your code is running correctly! Remember to give yourself sufficient time to go over anything you still have questions about or haven't figured out to make sure it will knit and submit properly. Comment your code! 


<br>

### Due Tuesday, Aug. 31th {-}

Complete Chapter 4 "Functions, Object, Vectors - oh my" in our lab manual before class; the focus is for you to feel a little more comfortable sending code from an `Rscript` (for most of the semester this will actually be an `Rmarkdown` instead but each code - chunk is a mini `Rscript`), and learning to comment your code. You should have a complete Rscript by next class, we'll talk briefly about any questions.

I will show you how to compile a report using the little notebook icon in the `Rscript` panel and submit the resulting `*.html` file through canvas.


<br>

### Due Wednesday, Aug. 25th (1pm) {-}

Introduce yourself in the  `#general` slack channel:

* Give your name, major, year
* Tell us what you are hoping to get out of this class^[If it's 'check it off my list to graduate' - that's fine, we'll try to make this class more than a check mark!].
* Let us know how much experience you have with data analysis and coding.

<!--chapter:end:index.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

# load libraries
library(msmbstyle)
library(knitr)

library("sp")
library("raster")
library("maptools")
library("rgdal")
library("dismo")
library("tidyverse")

# global settings for code chunks
knitr::opts_chunk$set(tidy = FALSE, 
                      message=FALSE, 
                      warning=FALSE,
                      cache.extra = packageVersion('msmbstyle'))

# invalidate cache when the tufte version changes
options(htmltools.dir.version = FALSE)

```

# What even is data science?

**Learning Objectives**

After completing this lab you should be able to 

* define what data is and what the structural/functional relationships to information, knowledge, insight and wisdom are.
* describe what data science is and the major field/skill sets that comprise it.
* describe the individual components that comprise the data science life cycle.

<br>

## Defining "data" and "science"

Data are qualitative and quantitative observations that are measured and collected. 

`r msmbstyle::question(label = "ques:ds-1", text = "Data fall in two distinct categories, categorical and numerical data. Briefly compare and contrast these two categories by describing the data types you would expect to find in each.")`

The '**data - information - knowledge - wisdom pyramid**' gives us a framework to consider how data can be used to inform decision making and impact the world around us. Not until **data** is organized and processed thus adding context can we glean **information** from the signal. Additional meaning is transferred as we synthesize and further contextualize information resulting in **knowledge**.

These three categories look backwards - we describe the "what" and ask about the "why" to reveal patterns and relationships. At this point we start looking forward to determine what action(s) should be taken, we know seek to reveal principles and directions that can be applied.

We integrate knowledge across disciplines to gain **insight** and **wisdom** to further understanding of problems and derive actionable solutions. This culminates in the decision-making process resulting in change.

`r msmbstyle::question(label = "ques:ds-2", text = "The science council defines science as the pursuit and application of knowledge and understanding of the natural and social world following a systematic methodology based on evidence. Compare and contrast this definition to the DIKW framework and make an argument that all science is data science.")`

##  What even is data science?

Data science is a fuzzy term and no single definition exists. Most definitions emphasize that it is a interdisciplinary field and that it has arisen in response to the increasingly large data sets that are produced. 

A common way of defining data science is to describe it as being the intersection of **domain knowledge**, **statistics/mathematics**, and **computer science** - though different definitions will ascribe more importance to certain different components.

One distinction to the typical scientific process as you may have learned it to be is that a large component of data science is **hypothesis generation** through exploratory analysis rather than **hypothesis confirmation**. 

The data science process generally starts by posing an interesting question and ends with visualizing and communicating the results. The key steps to the end results are **obtaining** the data, **processing** and **exploring** the data, and **modeling** the data to understand the data set and derive conclusions.

## The data science process

### Ask an interesting question

Let's start by asking an interesting question:

> Will Sasquatch by impacted by climate change?

Specifically, climate change could result in a shift in habitat availability, i.e. should we expect a species extinction because of habitat loss or a range expansion or range shift?

To do this we need to generate a species distribution model (SDM) for the Sasquatch, a large, hairy, bipedal ape-like creature found (or is it?) throughout North America^[I suppose, technically, since we were venturing into cryptozoology here it is not a species, but rather a cryptid distribution model], i.e. first we need to understand where Sasquatch are currently distributed to then assess how that might change in the future.

### Get occurrence data 

The first thing we need for any SDM is a data set documenting species occurrence, i.e. geo-coded observations of a given species in the wild.

Four our purposes, we turn to the Bigfoot Field Researchers Organization ([BFRO](https://www.bfro.net/)), founded in 1995 as the "*only scientific research organization exploring the Bigfoot/Sasquatch mystery*". You can turn to their website for answers on important [FAQs](https://www.bfro.net/gdb/show_FAQ.asp?id=405), including 'Do Bigfoot Sasquatch bury their dead?', 'Where is the physical evidence?', 'Wasn't this all shown to be a fake?, and 'Why do you want scientists to recognize the Sasquatch as a species? Isn't it better to just leave them alone?'. Their main focus though is on compiling reports of sightings and investigating them. In other words, they have a database full of geo-coded reported sightings^[And even better, it has already been downloaded and wrangled and is accessible (with sightings through 2018) right [here](https://data.world/timothyrenner/bfro-sightings-data).].

Let's read in the data and then we can take a look at the information we can glean from this data set by looking at the column names.

```{r}

# read data
occurrence <- read_delim("data/bfro_reports_geocoded.txt", delim = "\t")

```


### Tidy, Transform & Explore the data

Let's start by taking a look at our data set to determine how we need to wrangle to get the information we need process it so we can generate our species distribution model.

Because we need to be able to identify the exact locations Sasquatch occur, we are going to remove any observations that do not have latitude and longitude information.

```{r}

occurrence <- occurrence %>%
  filter(!is.na(longitude),
         !is.na(latitude))
         
```

Next, let's consider is what geographic extent of the observations is by looking at the distributions on a map.

```{r fig.cap="Map of Sasquatch sighting in the United States based on the BFRO database (1950 - 2021).", fig.width=8, fig.height=8}

# get minimum and maximum lat/longs
max.lat <- ceiling(max(occurrence$latitude))
min.lat <- floor(min(occurrence$latitude))
max.lon <- ceiling(max(occurrence$longitude))
min.lon <- floor(min(occurrence$longitude))

# create an extent object of the range of observations
geo_range <- extent(x = c(min.lon, max.lon, min.lat, max.lat))

# get base map
data(wrld_simpl)

# plot the base map
plot(wrld_simpl, 
     xlim = c(min.lon, max.lon),
     ylim = c(min.lat, max.lat),
     axes = TRUE, 
     col = "grey75")

# add individual occurrences
points(x = occurrence$longitude, 
       y = occurrence$latitude, 
       col = "darkorange", 
       pch = 20, 
       cex = 0.75)

# draw box around figure
box()

```

Here is an example of where domain knowledge comes in - while we are tidying and exploring the data set we need to assess whether there are artifacts our outlier data points that should be removed.

`r msmbstyle::question(label = "ques:ds-3", text = "Give a brief descriptions of the spatial distribution of the occurrence of Sasquatch in the United States. Note areas where sightings appear to be random, clustered or more dispersed, determine if you think any points should be removed.")`

`r msmbstyle::question(label = "ques:ds-4", text = "Discuss some possible explanations for the patterns you have observed to determine whether you think this data set is a reasonable representation of the ecological niche of the Sasquatch and can be used to create a species distribution model.")`

`r msmbstyle::solution_begin()`

Good catch - those sightings in the middle of the ocean are probably errors in the geo-coding. Further, we don't know how reliable the data for Alaska is as this data set is based on opportunistic sightings^[Well, admittedly if you poke around the BFRO website you will find that people do plan expeditions.]. 

The data for for observations in the lower 48 seems much more reliable, so let's restrict our data to those observations.

```{r fig.cap="Map of Sasquatch sighting in the lower 48 states based on the BFRO database as (1950 - 2021).", fig.width=8, fig.height=8}

# load and filter data
occurrence <- read_delim("data/bfro_reports_geocoded.txt", delim = "\t") %>%
  filter(!is.na(longitude),
         !is.na(latitude),
         longitude > -130,
         latitude < 55)

# get minimum and maximum lat/longs
max.lat <- ceiling(max(occurrence$latitude))
min.lat <- floor(min(occurrence$latitude))
max.lon <- ceiling(max(occurrence$longitude))
min.lon <- floor(min(occurrence$longitude))

# create an extent object
geo_range <- extent(x = c(min.lon, max.lon, min.lat, max.lat))

# get base map
data(wrld_simpl)

# plot the base map
plot(wrld_simpl, 
     xlim = c(min.lon, max.lon),
     ylim = c(min.lat, max.lat),
     axes = TRUE, 
     col = "grey75")

# add individual occurrences
points(x = occurrence$longitude, 
       y = occurrence$latitude, 
       xlim = c(min.lon, max.lon),
       ylim = c(min.lat, max.lat),
       col = "darkorange", 
       pch = 20, 
       cex = 0.75)

# draw box around figure
box()

```

`r msmbstyle::solution_end()`

This should work - on to the next step!


### Get more data

Occurrence data along will not allow us to build a species distribution model; what we need is environmental data to go with the locations, i.e. we need a set of predictor variables. 

Combining multiple data sets is typical for data science projects, frequently the quality of a project hinges on the availability of hight quality data sets that can provide information to describe or preodict behavior if the data set you are exploring.

Climate is complex and multidimensional, though at its core climate is determined by [long-term patterns in mean and variability of temperature and precipitation](https://openoregon.pressbooks.pub/envirobiology/chapter/3-3-terrestrial-biomes/). 

We are going to use the bioclim data set from [CliMond](https://www.climond.org/BioclimRegistry.aspx#Table1). Bioclim variables are commonly used for species distribution modeling as they are based on long-term trends (as opposed to e.g. the exact conditions when the species was observed). 

`r msmbstyle::question(label = "ques:ds-5", text = "Look up the descriptions of the [bioclim variables](https://www.climond.org/BioclimRegistry.aspx#Table1) and give a brief description of the four abiotic parameters that are included and how they are being parameterized. Argue which you think are most important to describe a species distribution/ecological niche and whether you think overall this data set will help us understand Sasquatch species distribution.")`

The data set we are using includes a core set of 19 variables that describe temperature and precipitation, along with an additional 16 variables that describe solar radiation and soil moisture. This information is encoded in the raster files of the historical (contemporary) bioclim data sets at a resolution of 10' (minutes) into the data folder. The "historical" data set consists of data from 1961 - 1990 centered on 1975. 

A raster file is an image file consisting of pixels with data associated with it. In this case, our "pixels" are 10' x 10' and depending on the layer the value associated with each pixel is the value for that bioclim value at that geographic location.

```{r}

# get list of files
files <- list.files("data/", pattern='^CM10_1975H', full.names=TRUE )

# import and convert to raster stack
predictors <- stack(files)

```

We have now created an object that at its core consists of a list where each element is a layer (bioclim variable raster).

### Tidy, transform & explore the data (again)

Let's plot the first bioclim variable (Bio01, annual mean temperature).

```{r fig.cap="Global distribution of annual mean temperature (1961 - 1990, centered on 1975).", fig.width=8, fig.height=8}

plot(predictors@layers[[1]])

```

We see the pattern we would intuitively expect, with temperatures decreasing as you move poleward and being warmest around the poles.

Let's extract the values for each bioclim variable at our occurrence points (observations).

```{r}

# create df with just xy coordinates
xy <- occurrence %>%
  dplyr::select(longitude, latitude)

# crop bioclim data to geographic range
cropped_predictors <- crop(x = predictors, y = geo_range)

# extract values
presence <- raster::extract(cropped_predictors, xy)

```

Let's take a quick look at the first few rows and columns of the matrix we just created.

```{r}

head(presence[,1:3])

```

`r msmbstyle::question(label = "ques:ds-6", text = "Briefly state what these values represent.")`

### Create species distribution model

Our next step is to fit a bioclim model to the data set we just generated.

```{r}

# fit bioclim model
model.fit <- bioclim(presence)

```

The bioclim model is a classic climate-envelope-model^[You may remember reading about this in your reading assignments.]. 

Briefly, the algorithm computes the similarity of locations by comparing the value of each environmental variables being used (our bioclim data set) to a distribution of that values at all locations with known presence^[Also called the training sites; these are our occurrence points.]. The closer that value is to the median (50th percentile), the more suitable that location is assumed to be. Suitability scores are between 0 and 1, with 1 indicating a "perfect suitability".)

In general, there is no distinction between the tails of the distribution (i.e. the 90th and 10th percentile are equivalent), though in some implementations you can specify those to be treated as distinct. As a result e.g. low levels of precipitation could be limiting but high levels would not be.

Finally, we will use our suitability scores and the bioclim raster data set to generate a predictive map of the Sasquatch species distribution. This means that the algorithm will assign a suitability score to each pixel based on the model and create a new raster layer.

```{r}

# generate raster with predicted distribution
prediction <- dismo::predict(x = cropped_predictors, 
                             object = model.fit,
                             ext = geo_range)

```

Let's plot our species distribution map.

```{r fig.cap="Species distribution model for Sasquatch. Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.", fig.width=8, fig.height=8}

# plot model probabilities
plot(prediction,
     xlim = c(min.lon, max.lon),
     ylim = c(min.lat, max.lat),
     axes = TRUE)

# add map
plot(wrld_simpl, add = TRUE, border = "black")

# draw box around it
box()

```

`r msmbstyle::question(label = "ques:ds-7", text = "Give a brief description of the habitat suitability map, including the maximum probabilities. Describe wether this result fits your expectations based on the distribution map of reports we looked at earlier. According to our map, if you were planning a Sasquatch research trip, where would you be headed?")`

Our highest habitat suitability values (probability of occurring) seem pretty low. One reason for this is that we used presence-only data.

`r msmbstyle::question(label = "ques:ds-8", text = "The alternative to presence-only models is to have presence-absence data. Discuss how this would improve the models. Argue why you think presence-only data sets are easier to generate.")`

`r msmbstyle::question(label = "ques:ds-9", text = "A work-around of not having absence data is to generate pseudo-absence data. This is done by generating random points within the geographic range and using those as proxies for absence data. Briefly argue the merits and limitations of such an approach.")`


### Model future climate change

Projections of future climate change are heavily dependent on human activity and the resulting greenhouse gas emissions. Therefore the IPCC's Assessment Reports contain scenario families that represent projected climate conditions based on emission scenarios resulting from future technological and economic development as defined by each scenario.

Let's look at how the species distribution map might change in response to a shift in environmental parameters.

To do this we will use bioclim raster files for 2100 generated using the A1B and A2 scenarios. 

The A1 climate scenarios assume a more integrated world characterized by rapid economic growth, a global population that peaks at 9 billion (2050) and the gradually declines, rapid spread of new/efficient technologies, and a convergent world characterized by extensive worldwide social and cultural interactions. Scenario A1B further assumes a balanced emphasis on fossil and non-fossil fuels. 

By contrast, A2 scenarios assume a more divided world consisting of independently operating and self-reliant nations and regionally-oriented economic development. The population is assumed to continuously grow. Finally, this scenario is characterized by high emissions.

Let's start with the **A1 climate scenario** to create our species distribution model.

```{r}


# get list of files
files <- list.files("data/", pattern='^CM10_2100_A1B', full.names=TRUE )

# import and convert to raster stack
predictors_A1 <- stack(files)


```

Now let's fit our model and create predictive map.

```{r}

# create df with just xy coordinates
xy <- occurrence %>%
  select(longitude, latitude)

# crop bioclim data to geographic range
cropped_predictors <- crop(x = predictors_A1, y = geo_range)

# extract values
presence <- raster::extract(cropped_predictors, xy)

# fit the bioclim model
model.fit <- bioclim(presence)

# create raster layer of predicted distribution
prediction <- dismo::predict(x = cropped_predictors, 
                             object = model.fit,
                             ext = geo_range)

```

Finally, let's plot our species distribution map.

```{r fig.cap="Predicted species distribution for Sasquatch in the lower 48 in 2100 (climate scenario A1B). Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.", fig.width=8, fig.height=8}

# plot model probabilities
plot(prediction,
     xlim = c(min.lon, max.lon),
     ylim = c(min.lat, max.lat),
     axes = TRUE)

# add map
plot(wrld_simpl, add = TRUE, border = "black")

# draw box around it
box()

```

`r msmbstyle::question(label = "ques:ds-10", text = "Give a brief description of the habitat suitability map, including the maximum probabilities. Describe whether this fit your expectations based on the change in bioclim variables. Describe the shift in habitat suitability relative to our current model.")`

How about our A2 climate scenario?

```{r fig.cap="Predicted species distribution for Sasquatch in the lower 48 in 2100 (climate scenario A2). Color indicates the probability of encountering Sasquatch in the lower 48 states based on habitat suitability.", fig.width=8, fig.height=8}

# get list of files
files <- list.files("data/", pattern='^CM10_2100_A2', full.names=TRUE )

# import and convert to raster stack
predictors_A2 <- stack(files)

# create df with just xy coordinates
xy <- occurrence %>%
  select(longitude, latitude)

# crop bioclim data to geographic range
cropped_predictors <- crop(x = predictors_A2, y = geo_range)

# extract values
presence <- raster::extract(cropped_predictors, xy)

# fit the bioclim model
model.fit <- bioclim(presence)

# create raster layer of predicted distribution
prediction <- dismo::predict(x = cropped_predictors, 
                             object = model.fit,
                             ext = geo_range)

# plot model probabilities
plot(prediction,
     xlim = c(min.lon, max.lon),
     ylim = c(min.lat, max.lat),
     axes = TRUE)

# add map
plot(wrld_simpl, add = TRUE, border = "black")

# draw box around it
box()

```

`r msmbstyle::question(label = "ques:ds-11", text = "Briefly compare this model to the previous two. Comment on whether you expected to see greater or smaller difference to the other future climate prediction based on the scenarios that they are based on.")`

`r msmbstyle::question(label = "ques:ds-12", text = "Discuss whether you think bioclim variables are good models to predict a species' respone to climate change. In your discussion consider how future bioclim data sets are generated, as well as, whether abiotic conditions along will determine range changes.")`

## Visualize and Communicate your results

Printing our maps side by side for better comparison would be a good way to visualize and communicate our results. We would probably include a discussion of our our approach (we should probably validate our model too which we haven't done here) and make recommendations based on our findings.


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

## Acknowledgments

Since I'm not the first person to create a lab/tutorial on species distribution modeling, I drew inspiration from various educators, R and data enthusiasts to shape this tutorial, most notably:

Anna L. Carter. November 2017, posting date. Painting turtles: an introduction to species distribution modeling in R. Teaching Issues and Experiments in Ecology, Vol. 13: Practice #1 [online]. http://tiee.esa.org/vol/v13/issues/data_sets/carter/abstract.html

Wendy L. Clement, Kathleen L. Prudic, and Jeffrey C. Oliver. 16 August 2018, posting date. Exploring how climate will impact plant-insect distributions and interactions using open data and informatics. Teaching Issues and Experiments in Ecology, Vol. 14: Experiment #1 [online]. http://tiee.esa.org/vol/v14/experiments/clement/abstract.html

https://jcoliver.github.io/learn-r/011-species-distribution-models.html


<!--chapter:end:01_IntroDataSci.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('msmbstyle'))
options(htmltools.dir.version = FALSE)

```


# Installing R/Rstudio


**Learning Objectives**

After completing this activity you should

* be able to download and install `R` and `Rstudio` on your laptop
* be able to install `Rtools` & `devtools` to be able to compile `R packages` from source (Windows).
* understand the main use for each of the four main panes in the `Rstudio` GUI.
* understand what a package is in `R` and how to install them.


## Install & Set up R and Rstudio on your computer

If you have already installed `R` and `Rstudio` make sure your R version is up to date. Whenever you open `Rstudio` the version will be printed in the console (bottom left pane). In addition, you can always check what version is installed by typing `sessionInfo()` into your console. You should be using version 4.0.0 or later. You do not need to uninstall old version of `R`. If you do have to update, you will need to re-install packages (see below) for R4.0.0

### Windows

**Install R**

* Download most recent version of R for Windows [here](https://cran.r-project.org/bin/windows/base/release.htm).
* Run the `.exe` file that was downloaded and follow instructions in the set-up wizard.


**Install Rtools**

* Download Rtools [here](https://cran.r-project.org/bin/windows/Rtools/rtools40-x86_64.exe).
* Run the downloaded `.exe` file that was download and follow the instructions in the set-up wizard.


**Install Rstudio**

* Go to Rstudio download (page)[http://www.rstudio.com/ide/download/desktop].
* Scroll down to select the `Rstudio` current version for Windows XP/Vista/7/8/10.
* Run the `.exe` file that was downloaded and follow instructions in the set-up wizard.


**Finish setting up Rtools**

* Open `Rstudio` to make sure you aren't getting any error messages.
* Put `Rtools` in your path by typing `writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")` in the console window.
* Install the `devtools` package by typing `install.packages("devtools")` in the console.


### Mac OS X

**Download & install R**

* Go to (CRAN)[http://cran.r-project.org/], select `Download R for (Mac) OS X`.
* Download the `.pkg` file for your OS X version.
* Run the downloaded file to install `R`.


**Download & install XQuartz** (needed to run some R packages)

* Download [XQuartz](https://www.xquartz.org/)
* Run the downloaded file to install


**Download & install Rstudio**

* Go to Rstudio download [page](http://www.rstudio.com/ide/download/desktop).
* Scroll down to select the `Rstudio` current version for Mac OS X.
* Run the `.exe` file that was downloaded and follow instructions in the set-up wizard.



## Get to know Rstudio

`Rstudio` is an Integrated Development Environment (IDE) that you can use to write code, navigate files, inspect objects, etc. The advantage of using an IDE is that you have access to shortcuts, visual cues, troubleshooting, navigation, and autocomplete help.


### GUI Layout

Open `Rstudio` and identify the four panes in the interface (default layout).

1. **Editor** (top left): edit scripts/other documents, code can be sent directly to the console.
2. **R console** (bottom left): Run code either by directly typing the code or sending it from the editor pane.
3. **Environment/history** (top right): Contains variables/objects as you create them & full history of functions/commands that have been run.
4. **files/plots/packages/help/viewer** (bottom right): Different tabs in this pane wil let you explore files on your computer, view plots, loaded packages, and read manual pages for various functions.

The panes can be customized (`Rstudio -> Preferences -> Pane Layout`) and you can move/re-size them using your mouse.


### Interacting with R in Rstudio

Think of `R` as a language that allows you to give your computer precise instructions (code) to follow.

* **Commands** are the instructions we are giving the computer, usually as a series of functions.
* **Executing** code or a program means you are telling the computer to run it.

There are three main ways to interact with `R` - directly using console, script files (`*.R`), or code chunks embedded in R markdown files (`*.Rmd`).

The console is where you execute code and see the results of those commands. You can type your code directly into the console and hit `Enter` to execute it. You can review those commands in the history pane (or by saving the history) but if you close the session and don't save the history to file those commands will be forgotten.

By contrast, writing your code in the script editor either as a standard script or as a code chunk in an Rmarkdown file allows you to have a reproducible workflow (future you and other collaborators will thank you).

Executing an entire script, a code chunk, or individual functions from a script will run them in the console.

* `Ctrl` + `Enter` will execute commands directly from the script editor. You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute.
* If you are using an Rmarkdown file you can execute an entire code chunk by pressing the green arrow in the top right corner.

If the console is ready for you to execute commands you should see a `>` prompt. If you e.g. forget a `)` you will see a `+` prompt - `R` is telling you that it is expecting further code. When this happens and you don't know what you are missing (usually it is an unmatched quotation or parenthesis), make sure your cursor is in the console and hit the `Esc` key.

*We will run through these options, but you can always check back here while you are getting used to R*.


### Customize Rstudio

There are several options to customize Rstudio including setting a theme, and other formatting preferences. You can access this using `Tools > Global Options`. I recommend using a dark theme (it's a lot easier on the eyes) and keeping the panes in the same positions as described because it will make troubleshooting a lot easier^["You should see xx in the top left" is a lot more helpful if your top left looks like my top left!].


## Installing and using packages in R

### Install a package

Think of R packages as tool kit comprising a set of functions (tools) to perform specific tasks. `R` comes with a set of packages already installed that gives you base `R` functions; you can view these and determine which have been loaded in the `Packages` tab in the bottom right pane. For other tasks we will need additional packages. ^[Most `R` packages are found in the [CRAN repository](https://cran.r-project.org/) and on [Bioconducter](https://www.bioconductor.org/), developmental packages are available on [github](https://github.com/).]

A central group of packages for data wrangling and processing form the [tidyverse](https://www.tidyverse.org/), described as "... an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures." - We are going to heavily rely on core functions from the tidyverse to wrangle, summarize, and analyze data. 

When you install packages they will be downloaded and installed onto your computer. Determine what your default path is using `.libPaths()` and change if necessary.

The easiest way to install packages is to use the `install.packages()` function. Use the `R` console to install some libraries to get us started (we will install other libraries as needed for other labs).

```{marginfigure}

Using `#` in an R script allows you to insert comments that are ignored by R when executing your code. Use comments to document your code, future you will thank you!

```

```{r eval=FALSE, echo=TRUE}

# install the rmarkdown package
install.packages("rmarkdown")

# install central packages in the tidyverse
install.packages("tidyverse")

# install additional packages
install.packages("plyr", "ggthemes", "patchwork", "glue")

```

Let's check if you were able to successfully install those packages.

```{r eval=FALSE, echo=TRUE}

# load library
library(tidyverse)

```

If you don't see any error messages in the console along the lines of `there is no package called ...` you are all set. If you look in the `packages` tab in the lower right panel you should also see that packages such as `dplyr` and `tidyr` (two of the central `tidyverse` packages) now have a little check box next to them.


### Updating R packages

You should generally make sure to keep your `R` packages up to date as new versions include important bugfixes and additional improvements. The easiest way to update packages is to use the `Update` button in the `Packages` tab in the bottom right panel.

**However** be aware that updating packages might break some code you have previously written. For most of what we will be doing this should not be the case. If you used `R` for a previous course, make sure to update you packages at the beginning of this course and we should be set for the semester.

<!--chapter:end:02_SetUpR.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('msmbstyle'))
options(htmltools.dir.version = FALSE)

```

# Project management and Rmarkdown Basics

**Learning Objectives**

After completing this tutorial you should

* be able to set up a well structured project.
* understand what a working directory is and how to set up an R project.
* understand the value of using Rmarkdown files to document your work.
* understand the core components of the markdown format.
* be able to use basic markdown syntax to format a document (headers, bold/italics).
* be able to add a code chunk to a `.Rmd` file and add options.
* understand how to modify code chunk options to determine what is/is not rendered in the knitted document.
* know how to `knit` a document to produce an `*.html` file from an `*.Rmd`.


The goal of open science and reproducible research is to make scientific methods, data, and results more transparent, available, and reproducible. `R markdown` files are a useful tool to be able to generate reports documenting your data, methods (code used to process data), and results. Key packages allowing you to do this are `knitr` and `rmarkdown`.
^[Download the R markdown cheat sheet (here)[https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf].]

`R markdown` (`*.Rmd`) files will let you document your workflow, share how you processed/analyzed your data and the resulting output along with any visualizations. `.Rmd` is a text based file format that consists of standard text, code chunks, and the resulting output using a very simple syntax (henc mark*down* as opposed to mark*up* languages like `html` or `LateX` which have much more complicated syntax). When you knit your document, the code is executed and the resulting output with be included in the rendered document (common formats are `html` or `pdf`). Advantages to a workflow centered around using `R markdown` to document your work include:

* the simple syntax makes it easy to learn the basics (but some of the more advance options will let you create some sophisticated reports).
* resulting files have a simple, standardized formatting that looks professional and is easy to read and understand (code, documentation, figures all in one place).
* future you will be thankful when you don't have to remember your assumptions, individual steps, and modifications.
* easy modification to extend/refine analysis or re-run with updated data.


## Project organization 101

A key component to doing data analysis is organizing your **working directory** which will contain your data, scripts, `Rmarkdown`-files, results, figures, and other output. Keeping this well organized will help you establish a reproducible workflow, collaborate well, and share your results.


### Organizing your files and directories

For each project/lab we will set up a project directory^[We'll use this term interchangeably with working directory and research compendium] with the following set of sub-directories:

* `data`
* `results`
* `scr`


### A note on Naming things

Naming conventions for files, sub-directories etc. should conform to the following key principles^[see Jenny Bryan's [excellent summary](https://speakerdeck.com/jennybc/how-to-name-files?slide=3) of these principles]:

* **Human readable**: keep it short but self-explanatory.
* **Machine readable**: don't use special characters of spaces.
* **Sortable**: standardize components of the file names to make it possible to sort files and find what you are looking for.


### Set up your project directory using Rprojects

Create a project directory
^[Yes, a directory is essentially a folder, however when using the term directory we are considering the relationship between a folder and it's full path.]
(e.g. `IntroR`) on your computer, within that directory create sub-directories `data`, `results`, `scr`.

Now, we are going to create an `R project` within this directory.

* in the top right hand corner click on the project icon
* select `New Project` and `Create in existing directory`; follow the prompts.

This should create a new `R project` and open it (the `R project` name should be in the top right corner next to the icon).

If you look in the bottom right hand pane in the `Files` tab, the bread crumbs should lead to your project folder which has now become your `working directory`, i.e. all paths are relative to this location. ^[If you weren't working with an `R project`, you can set your working directory by naviating to your new working director and selecting `More > Set as working directory`.] If you navigate away from your `working directory` (project directory) you can quickly get back to your project directory by clicking on the project icon in the `Files` pane or by clicking the cog icon (`More`) and selecting `Go to Working Directory`.


## Create your first .Rmd file

Create a new `.Rmd` file using `File -> New File -> R Markdown` and save that file in your project directory as **Lastname_IntroR.Rmd**.

We'll break down the components of the `.Rmd` file in just a second but to give you the full experience hit the `Knit HTML` button to render an `html` document.

Cool beans, now to the knitty-gritty.


## Structure of an Rmarkdown File

An `Rmarkdown`-file consists of three components:

1. **Header**: written in `YAML` format the header contains all the information on how to render the `.Rmd` file.
2. **Markdown Sections**: written in (Rmarkdown syntax)[https://rmarkdown.rstudio.com/authoring_basics.html].
3. **Code chunks**: Chunks of `R` code. These can be run interactively (e.g. while generating your document) and will be rendered when knitting the document.


## YAML header

The header is written in (YAML syntax)[https://en.wikipedia.org/wiki/YAML], it begins and ends with `---` with four default elements that will determine how the document is knitted.

```

---
title: "title"
author: "name"
date: "Date"
output: html_document
----

```

Customize your `.Rmd`:Change the title and add your name in the author line. Changing the date to `` `r "\u0060r Sys.Date()\u0060"` `` will automatically include the current date when you render the document. Leave the default output setting.


## Markdown sections

Your markdown sections can contain any text you want using the markdown syntax; once you render the `.Rmd` the resulting (`html`) file will appear as text.

Most of your text (without syntax) will appear as paragraph text but you can add additional syntax to format it in different ways. Check the official documentation for the [basics](https://rmarkdown.rstudio.com/authoring_basics.html).


## Code chunks

Code chunks contain your R code and start and end with three back ticks; `{r}` determines that the code chunk should be interpreted as `R` code.

```` ```{r} ````


```` ``` ````


You can add a code chunk using `Ctrl` + `Alt` + `I` or `Code -> Insert Chunk`. You can run code chunks line by line using `Ctrl` + `Enter` or you can run the entire chunk by clicking the `Run` button or the little green arrow in the top right corner of an `R` chunk.

Use `#` to comment your code, any lines following a `#` will not be run by R, you can use them to describe what your code is doing.

You can add options to each code chunk to customize how/if a chunk is executed and appears in the rendered output. These options are added to within the curly brackets. The three most common code chunk options are:

* `eval=FALSE`: results in code chunk not being evaluated or run though it will still be rendered in the knitted document.
* `echo=FALSE`: This will hide the code in the output though the generated output will still be rendered for the knitted document.
* `results=hide`: This will result in the chunk being evaluated though neither the results or the code will be rendered in the knitted document.

Multiple options can be applied to the same chunk; find additional options (here)[http://yihui.name/knitr/demo/output/].


## Render your document using knitr

`knitr` is an `R` package used to render `R markdown` files to another format (usually `html` or `pdf`). In Rstudio the most straightforward way of knitting a document is using the `Knit HTML` button. This will open a new tab in your console title R Markdown that will show the knitting process; any errors that occur with show up here along with a line number so you can determine where the error is occurring in your `.Rmd` file to troubleshoot the issue. The output will automatically be saved in your working directory.


## Some advanced options

You can add images to an `R markdown` as follows ^[make sure the path to your image is relative to your `.Rmd` file; e.g create a `images` subdirectory in your project directory.]:

```

![alt text here](path-to-image-here)

```

You can stylize your rendered document by modifying the `YAML` header to include a table of contents like this^[the option of `toc_depth` determines how many levels are included in the table of contents, e.g. here headers at level 1 and 2 will be included]:

```
---
title: "A table of contents?!"
output:
  html_document:
    toc: true
    toc_depth: 2
---
```

If you really want to jazz things up, you can change the theme^[you can choose from various options [here](https://bootswatch.com/3/)].

```
---
title: "A snazzy theme?!"
output:
  html_document:
    theme: flatly
    toc: true
    toc_depth: 2
---
```

For being so simple, `Rmarkdown` is quite powerful - how powerful you ask? This lab manual was written in `Rmarkdown`.

<!--chapter:end:03_Rproj.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# R: Functions, Objects, Vectors - oh my!

**Learning Objectives**

After completing this tutorial you should be able to

* name, create, and assign values to objects.
* save a series of commands/code as an `R script`.
* use comments to describe your code/scripts.
* call `functions` and modify the default options using their `arguments`.
* understand what a `vector` is and distinguish between the main data types.
* to inspect, subset, and extract their content from a `vector`. 
* understand how `data.frames` and `vectors` relate.



## R is all about `Objects`

You can think of the `R console` as a super-powerful calculator. You can get output from `R` by simply typing math directly into the console.

```{r}

13 + 29

```

or 

```{r}

546 / 13

```

Well that's fun - but not super helpful. 

For us to get to the good stuff we need to assign `values` to `objects`. Creating an `object` is straightforward. First, we give it a name, then we use the `assignment operator` to assign it a `value`.

The `assignment operator` (`<-`) assigns the `value` on the right of the `<-` to the `object` on the left.

```{r}

fork_length_mm <- 344

```

Type that into the console and execute the command using `Enter`. If you look at your `Global Environment` (top right panel) you should now see `forklength` and the `value` you assigned it.

Notice, how when you assigned a `value` to your new `object` nothing was printed in the console compared to when you were typing in math.

To print the `value` of an `object` you can type the name of the object into the console.

```{r}

# print value
fork_length_mm

```

Now that `fork_length` is in your environment we can use it to compute instead of the `value` itself.

For example, we might need to convert our fork length from millimeters (mm) to centimeters (cm).

```{r}

fork_length_mm / 10 

```

We can change the `value` of an `object` any time by assigning it a new one. Changing the `value` of one `object` does not change the `values` of other objects.

```{r}

fork_length_mm <- 567

```


`r msmbstyle::question(label = "ques:R-1", text = "Create a new object with the fork length in centimeters. Then change then change the value of our fork length in millimeters object to 50. What do you think the value of fork_length_mm will be now?")`


**More thoughts on naming things**^[You will soon discover that coding is 90% naming things.]

Theoretically, we can name objects anything we want - but before that gets out of hand let's think about some guidelines for naming objects.

* Make them simple, specific, and not too long (otherwise you will end up with a lot of typing to do and difficulties remembering which object is which).
* Object names cannot start with a number.
* R is case sensitive, `fork_length` is not the same as `Fork_Length`.
* Avoid using dots (`.`) in names. Typically dots are used in function names and also have special meaning (methods) in R.
* Some names are already taken by fundamental functions (e.g. `if`, `else`, `for`) and cannot be used as names for objects; in general avoid using names that have already been used by other function names.
* Rule of thumb: nouns for object names, verbs for function names.

Using a consistent style for naming your objects is part of adopting a consistent styling of your code - this will make following your code a lot easier for yourself and others. This includes things like spacing, how you name objects, and upper/lower case.


## Saving your work

So far, we have inputed all of our code directly into the `console`. If you scroll up in the `console` you will find that all the commands and results from your current R session are still in the console. Using `Ctrl + L` will clear the entire console.

Uh-oh - what if we need to go back over the code we just cleared?

Well, for one if you check the `History` tab in the top right panel you will see that all your commands have been recorded. If you highlight one of them and either click on `To Console` or hit `Enter` it will send it directly to the console.

Usually your history will be saved automatically when you close R/end an R session (unless you have changed the settings) and it will be restored when you open R again. You can use the broom icon to clear your entire history.

Uh-oh - now what do we do?

In general, you should only be typing code directly into the console for quick queries but since usually we want to be able to revisit and share our work you will want to be able to save your work in an `R script` (`*.R`) or include it in an `Rmarkdown` file (`*Rmd`) as a code chunk. For this course we will mostly be operating with `Rmarkdown` files.

You can open a new `R script` using `Ctrl + Shift + C` or using `File > New File > R Script`. This will open an R script in a new tab in the top left pane.

Save your `R script` using `Ctrl + S` or `File > Save As` - this will open a dialogue box for you to save your `R script` with the file extension `.R`.

`Ctrl` + `Enter` will execute commands directly from the script editor You can use this to run the line of code your cursor is currently in in the script editor or you can highlight a series of lines to execute. You can also run all the code in a script by clicking on the `Run` button.

Create a new `R script` to keep track of the rest of the things we will learn today.


## Using comments

You can add comments to your `R scripts` using `#`. Essentially, once you type an `#` in a line anything to the right of it will be ignored.

This is really helpful as it will allow you to comment your script, i.e. you can leave yourself and other notes and explanations as to what your code is doing. This is especially helpful if you come back to some of your code after a period of time, if you are sharing your code with others, and when you are debugging code. You will find that as you become more experienced your comments will become shorter and more concise and you might even be tempted to leave them out completely - don't!

For example you might find a comment like this more helpful at the moment:

```{r}

# assign value to new object total length
fork_length <- 436

```

But soon you'll find this just as helpful:

```{r}

# total length fish
fork_length <- 436

```


`r question_begin(label = "ques:R-2")`

What do you expect the value of the object `total_length` to be after executing this command?

```{r}

FL <- 436  # total length fish

```

`r question_end()`

You can comment/uncomment multiple lines at once by highlighting the lines you want to comment (or uncomment) and hitting `Ctrl + Shift + C`. This can be useful if you are playing around with code and don't want to delete something but don't want it to be run either.


## Functions

When we installed `R packages` earlier we mentioned that they are sets of predefined `functions`. These are essentially mini-scripts that automate using specific sets of commands. So instead of having to run multiple lines of code (this can be 10s - 100s of lines code) you *call* the function instead.

Each function usually requires multiple inputs (*arguments*) and once executed return a `value` (though this is not always the case).

For example the function `round()` can be used to round a number^[This is an excellent example of naming things well!].

```{r}

fork_length_cm <- round(34.8821)

```

If we print the value of our object we see the following value is returned.

```{r}

fork_length_cm

```
For this function the input (argument) is a number and the returned value is also a number. This is not always the case, arguments can be numbers, objects, file paths ...

Many functions have set of arguments that alter the way a function operates - these are called `options`. Generally, they have a default value which are used unless specified otherwise by the user.

You can determine the arguments as function by calling the function `args()`.

```{r}

args(round)

```

Or you can call up the help page using `?round` or by typing it into the search box in the `help` tab in the lower right panel.

For example, our `round()` function has an argument called `digits`, we can use this to specify the number of significant digits we want our rounded value to have.

```{r}

round(34.8821, digits = 2)

```

If you provide the arguments in the exact same order as they are defined you do not have to specify them.

```{r}

round(34.8821, 2)

```

And you can switch their order if you do.

```{r}

round(digits = 2, x = 34.8821)

```

Good code style is to put the non-optional arguments (frequently the object, file path or value you are using) first and then specify the names of all the optional arguments you are specifying. This provides clarity and makes it easier for yourself and others to follow your code. Occasionally you might even want to use comments to further specify what each argument is doing or why you are choosing a specific option.

```{r}

round(34.8821,
      digits = 2)  # specify number of significant digits

```


## Vectors (data types I)

Now that we've figured out what objects and functions are let's get to know the two data types we will be spending the most time with this semester - `vectors` and data frames (`data.frame`)^[Other data types include lists (`list`), factors (`factor`) matrices (`matrix`),  and arrays (`array`); we'll introduce those later on.].

The most simple data type in R is the (atomic) `vector` which is a linear vector of a single type. There are six main types -

* `character`: strings or words.
* `numeric` or `double`: numbers.
* `integer`: integer numbers (usually indicated as `2L` to distinguish from `numeric`).
* `logical`: `TRUE` or `FALSE` (i.e. boolean data type).
* `complex`: complex numbers with real and imaginary parts (we'll leave it at that).
* `raw`: bitstreams (we won't use those either).

You can check the data type of any object using `class()`.

```{r}

class(fork_length)

```

Currently, our `fork_length` object consists of a single value. The function `c()` (concatenate) will allow us to assign a series of values to an object.

```{r}

fork_length <- c(454, 234, 948, 201)

fork_length

```

`r msmbstyle::question(label = "ques:R-3", text = "What data type do you expect this vector to be?")`

We call the same function to create a `character vector`.

```{r}

sharks <- c("bullshark", "blacktip", "scallopedhammerhead")

class(sharks)

```

The quotes around `"bullshark"` etc. are essential because they indicate that this is a character. If we do not use quotes, R will assume that we are trying to call an object.

```

bullshark

```

Since these objects don't exist, we will just get an error message.

You can use `c()` to combine an existing object with additional elements (assuming they are the same data type).

```{r}

species <- c(sharks, "gafftop")

species

```
Next to `class()` there are other helpful functions to inspect the content of a vector. For example `length()` will tell you how many elements are in a particular vector.

```{r}

length(fork_length)


```

The function `str()` will give you an overview of the structure of any object and its elements.

```{r}

str(fork_length)

```
Recall, that an atomic vector is a linear vector of *a single type* - what does that mean. Let's take a look at what happens if we create atomic vectors where we mix the data types.

```{r}

numeric_character <- c(1, 2, 3, "a")
numeric_logical <- c(1, 2, 3, TRUE)
character_logical <- c("a", "b", "c", TRUE)
wtf <- c(1, 2, 3, "4")

```

`r msmbstyle::question(label = "ques:R-4", text = "Describe what happens when data types are mixed in a single atomic vector?")`

We already discovered that we can combine vectors - but can we extract certain components from vectors? Indeed, there are a variety of ways that we can subset vectors.

The most simple way is using square brackets to indicate which element (or elements) we can't extract. In R, indices start at 1.^[This is not the case for all programming languages, e.g. Perl, Python, or C++ start with 0.]

```{r}

# extract second element
species[2]

# extract fourth and second element
species[c(4, 2)]

```
You can also repeat indices to create a new object with additional elements.

```{r}

species_longer <- species[c(2, 2, 4, 3, 4, 4, 1, 1)]

species_longer

```

More frequently, we will want to extract certain elements based on a specific condition (conditional subsetting). 

This is done using a logical vector, here `TRUE` select the element with the same index and `FALSE` will not.

```{r}

fork_length <- c(454, 234, 948, 201)

# use logical vector to subset
fork_length[c(TRUE, FALSE, TRUE, FALSE)]

```

This seems like a very impractical option but normally we would not create the logical vector by hand as we have done here, rather it will be the output of a function or logical test. For example, we might want to identify fish with a fork length > 300mm.

```{r}

# identify fish with fork length > threshold
fork_length > 300

```
We can use this to subset our vector directly.

```{r}

fork_length[c(fork_length > 300)]

```
There are a series of `boolean expressions`^[logical statements that are either true or false] we can use for subsetting vectors. Most of them you are probaly already familiar with because math.

* `>` and `<` (greater/less than)
* `=>` and `=<` (equal to or greater/less than)
* `==` (equal to) and `!=` (is not equal to)

You can combine to **boolean expressions** using `&`, (**both** conditions must be met) and `|` (at least one condition must be met).

`r question_begin(label = "ques:R-5")`

Let's give this a whirrl.

Subset the `fork_length` vector to 

* contain only values equal to 234
* contain all values but 948
* contain all values larger than 230 but smaller than 900
* contain all values smaller than 250 or larger that 900

`r question_end()`

R is set apart from other programming languages because it was designed to analyze data^[some people will argue that it is a 'statistical language' rather than a true programming language ... don't listen to them!] it has straightfoward ways to deal with missing data (`NA` or `na` values) because those are quite common in real world data sets.

Let's create a vector with a missing value.

```{r}

total_length <- c(560, NA, 1021, 250)

```

Let's say we want to calculate the mean value.

```{r}

mean(total_length)

```
Most functions will return `NA` when doing operations on objects with missing values. As such, many functions include an argument to omit missing values.

```{r}

mean(total_length, na.rm = TRUE)

```

Other functions that are helpful to deal with missing values are `is.na()`, `na.omit()`, and `complete.cases()`.

`r msmbstyle::question(label = "ques:R-6", text = "Run each of these functions on our total_length vector and describe what they do.")` 


## Data frames (data types II)

Recall that **atomic vectors** are *linear* vectors of a simple type, essentially they are one dimensional. Frequently we will be using data frames (`data.frame`) which you can think of as consisting of several vectors of the same length where each vector becomes a column and the elements are the rows.

Let's create a new object that is a dataframe with three columns containing information on species, fork length, and total length.

```{r}

# combine vectors into data frame
catch <- data.frame(species, fork_length, total_length)

```

You should now see a new object in your `Global Environment` and you will now also see that there are two categories of objects `Data` and `Values`. You will see that the `data.frame` is described as having 4 obs (observations, those are your rows) of 3 variables (those are your columns). If you click on the little blue arrow it will give you additional information on each column - note that because each column is essentially a vector, each one must consist of a single data type which is also indicated.

Calling the `str()` will give you the same information.

```{r}

str(catch)

```

You can further inspect the `data.frame` by clicking on the little white box on the right which will open a tab in the top left panel next to your R script. You can also always view a `data.frame` by calling the `View()` function.

```{r}

View(catch)

```

This can be a helpful way to explore your `data.frame`, for example, clicking on the headers will sort the data frame by that column. Usually we won't build or `data.frames` by hand, rather we will read them in from e.g. a tab-delimited text file - but more on that later.

<!--chapter:end:04_IntroR.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('msmbstyle'))
options(htmltools.dir.version = FALSE)

```

# Identifying essential fish habitat: Shark Nurseries on the Texas coast

**Learning Objectives**

After completing this activity you should be able to

* define essential fish habitats and shark nurseries.
* understand how differences in methodology can affect a data set.

For each of our modules we will have a project-folder with an `Rproject`, `*.Rmd`-files, and sub-directories for data, scripts, and results as described in our `Rproject` Tutorial. You should create a directory on your Desktop or Documents folder on your laptop (name it something like `BI449`) as a home directory for all of our project folders this semester. 

Download the [SharkNurseries](https://drive.google.com/drive/folders/1FexsfMkoBfPEeAkOCd2ZQrQqVBOF4WOR?usp=sharing) project folder. Once you have downloaded it, unzip the project directory into your `BI449` directory. You can open the `Rproj` for this module either by double clicking on it which will launch `Rstudio` or by opening `Rstudio` and then using `File > Open Project` or by clicking on the `Rproject` icon in the top right of your program window and selecting `Open Project`. 

Once you have opened a project you should see the project name in the top right corner^[Pro tip: If you run into issues where an Rmarkdown won't render or file paths aren't working (especially if things were working previously) one of your first steps should be to double check that the correct `Rproj` is loaded.].

You should have a `SharkNurseriesIntro.Rmd` in your project directory. Use that file to work through this tutorial - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. Remember to use comments to describe your code; at minimum you should have one comment per code set^[You should do this whether you are adding code yourself or using code from our manual, even if it isn't commented in the manual ... you may of course add as many comments as you need to be able to recall what you did].


## Essential fish habitat: Shark Nurseries

The [Magnuson-Stevens Act](https://www.federalregister.gov/documents/2002/01/17/02-885/magnuson-stevens-act-provisions-essential-fish-habitat-efh) (1996) defined essential fish habitat as "those waters and substrate necessary to fish for spawning, breeding, feeding or growth to maturity", i.e. they are habitats necessary for an organism to complete their life cycle. Identifying essential fish habitats is critical for management and conservation plans because it enables policy makers to prioritize certain ecosystems.

While some elasmobranchs (sharks, rays, skates) inhabit estuaries year round, many use the estuaries for specific purposes such as feeding, mating, gestation, parturition or as nurseries and only inhabit them during specific life history stages. Estuaries are heavily impacted by humans - overfishing, pollution, habitat destruction and altered flow regimes all affect the biological communities they support.

Broadly, shark nurseries are areas where young are born and/or reside in during maturation. Typically, these would areas that provide additional protection (e.g. mangroves for hiding) and plenty of food. Heupel et al. [@Heupel2007, Heupel2019] developed three criteria to defining shark nurseries as 

1. an area where sharks are commonly encountered within compared to outside of.
2. an area in which Young-of-the-year (YOY)/juveniles remain or return to for extended periods of time.
3. an area that is repeatedly used across years.

`r msmbstyle::question(label = "ques:sh-1", text = "Consider how you could design a study to identify estuaries that are shark nurseries.")`


## Identifying shark nurseries on the Texas Coast

Texas Parks and Wildlife (TPWD) defines eight major estuaries along the Texas coastline and performs regular shore-based gill net surveys for 10 week periods in April - June and September to November. Analysis of this survey has identify eight elasmobranch species [@Plumlee2018].

* Bull shark
* Bonnethead
* Cownose ray
* Blacktip shark
* Atlantic stingray
* Atlantic sharpnose shark
* Spinner shark
* Scalloped hammerhead 
* Finetooth shark
* Lemon shark

Gill nets generally exclude individuals > 2m.

More recently, a multi-year open water long-lining study targeting elasmobranchs was performed in three estuarine locations near Corpus Christi, TX that are considered putative shark nurseries. Here, the sampling period lasted from May to November [@Swift2020].

`r msmbstyle::question(label = "ques:sh-2", text = "Discuss whether or not you would expect to get similar results from both studies and what factors could result in differences.")`

`r msmbstyle::solution_begin()`

Here are some things to consider: 

* gear-bias (hook size, net-size)
* seasonality (peak use for many coastal elasmobranchs is May - Oct)
* spatial (land-based vs open ocean)

`r msmbstyle::solution_end()`

This study wanted to answer four questions to further understand whether these locations should be considered shark nurseries and therefore as essential fish habitat for specific elasmobranch species.

1. How does the composition of elasmobranch communities compare across sites?
2. How does the catch-per-unit-effort (CPUE) per species and life history compare across sites?
3. What do the sex ratios look like?
4. What environmental predictors can we use to predict presence of elasmobranchs?


## Explore the data set

There should be a an excel file in the `data` folder of your project directory named `longline_catchdata.xlsx`. Go ahead and open the file in excel.

`r msmbstyle::question(label = "ques:sh-3", text = "Take a look at the excel file and describe how you would figure out what the dimensions are (how many rows? columns?) and see if you identifying what information is contained in each column.")`

`r msmbstyle::solution(text = "The data set contains information on the sampling site, Species name, Sex, life history stage, Pre-caudal length (PCL), fork length (FL), stretch total length (STL), Hook Size, and the set number and sampling date.")`

While some column headers were probably more easy to identify than others a good data set comes with meta data - this is data about the data. At minimum it should contain detailed information on what is contained in each column, frequently it may also contain information on how certain data was calculated/generated.

Now that we're looking at the data set we might as well take a stab at the first question.

> How does the composition of elasmobranch communities compare across sites?

`r msmbstyle::question_begin(label = "ques:sh-4")`

Use your excel skills to create sets of summary tables

* Total catch by site:
    + total number of individuals per elasmobranch species caught at each site (across all years).
    + relative abundance of each elasmobranch species caught per site.
* Differences in catch across sites and years:
    + total number of individuals per elasmobranch species per site and year.
    + relative abundance per of each elasmobranch species per site and year.
* Summary stats size:
    + mean size of sharks (exclude rays) by site.
    + minimum size per shark species by site.
    + maximum size per shark species by site.

`r msmbstyle::question_end()`

`r msmbstyle::question(label = "ques:sh-5", text = "Imagine that just as you finished creating these summary stats tables your lab mate comes in tells you that they just discovered three data sheets in a drawer that wasn't entered in with the other data. How easy would it be to update your results tables? Consider disadvantages of using Excel to analyze your data in this way.")`


## Export as text file

Our next step is to learn how to use `R` to wrangle data, this can include both cleaning up your data to put it into a standard format or analysis as well as using a raw data set to generate a data sets based of off them for further analysis or summarize results.

Two of the most common file formats you are going to encounter when you access or download data sets are `*.csv` (comma-delimited files) and tab-delimited files (`*txt`); both of these are straightforward to read into `R`. You can convert your excel file into these formats by using `File > Save As` and then selecting the appropriate file formats.

`r msmbstyle::question(label = "ques:sh-6", text = "Go ahead and save a csv and tab-delimited formatted version of our data set in your data folder. Then open those files in a text editor and describe the difference between the two formats and briefly discuss the advantages of storing large data sets in these formats over a typical excel file.")`

<!--chapter:end:05_SharkNurseriesIntro.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
    includes:
     in_header: header.html
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Intro to data frames

**Learning Objectives**

After completing this tutorial you should be able to

* read a data from a csv or tab-delimited file into R as a `data.frame`.
* describe what a `data.frame` is.
* inspect the contents of a data frame to summarize the content.
* write a `data.frame` to file.

You should have a `DataFrames.Rmd` in your project directory. Use that file to work through this tutorial - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. Remember to use comments to describe your code; at minimum you should have one comment per code set^[You should do this even if you are adding code yourself or if you are using code from our manual, even if it isn't commented in the manual ... you may of course add as many comments as you need to be able to recall what you did].

Let's start by loading the packages we will need for this activity.

```{r}

# load libraries
library(knitr)
library(tidyverse)

```


## Reading data into R

`library(tidyverse)` is actually loading a set of packages used for data science that share a common design philosophy, and "grammar". One of the packages we loaded is called `readr` which contains functions for reading in and parsing files.

`r msmbstyle::question(label = "ques:df-1", text = "Use ?read_delim to pull up the help page for the function we will using and explore the arguments. How do you think we read in our csv file?")`

`r msmbstyle::solution_begin()`

`read_delim` has two required arguments, the **path** (`data/longline_catchdata.csv`) which tells `R` where your file is located and the **delimiter** in this case a comma (`,`) tells `R` how columns are separated from each other.

```{r eval=FALSE}

# read catch data
catch <- read_delim("data/longline_catchdata.csv", delim = ",")

catch %>%
  distinct(Species)

```

As we access data sets that are not as "clean" as the one we have hear, you will find that some of the other arguments will come in handy - but don't worry about those for now.

`r msmbstyle::solution_end()`


Execute the code. If you look over in your environment pane you should now see the object `catch`. This is your dataframe. Click on it, you should see the command `View(catch)` in your console and a tab `catch` appear in your top left pane.

`r msmbstyle::question(label = "ques:df-2", text = "Based on how you read in the csv file how would you read in the tab-delimited version?")`

`r msmbstyle::solution_begin()`

We need to change delimiter - `\t` is "computer" for tab. 

```{r}

# read catch data
catch <- read_delim("data/longline_catchdata.txt", delim = "\t")

```

Another delimiters you might encounter are white space (`" "`) but technically it could be anything. 

`r msmbstyle::solution_end()`

When you loaded your data set you should have seem an message along the lines of `parsed with column specification` and information on the number of columns and their data type. What this means is that `read_delim()` looks through the first 1,000 rows for each column and guesses the data type - usually this works pretty well though occasionally we will have to either specify the data types manually (`col_types` argument) or convert the data type later on.

Let's use `class()` to figure out what type of object we are dealing with.

```{r}

class(catch)

```
You can see that this object actually has multiple classes attached to it. The last one in the list is `data.frame` which is the standard format for (rectangular) tabular data.

Recall from our tutorial on vectors that each column in a `data.frame` is an atomic vector, they must all have the same length (hence, "rectangular") and each must contain the same data type (character,s integers, ...).

The other three have the same basic properties as a `data.frame` along with some additional features. The `tbl` (pronounced tibble) was designed to be at the center of the `tidyverse` which means that when you use `readr` functions it will automatically be read in as a tibble and `data.frame`. If you do some exploring and/or troubleshooting on the web you will likely run into tibbles but for our intents of purposes we will use `data.frame`.



## Inspect your `data.frame`

`r msmbstyle::question_begin(label = "ques:df-3")`

There are several functions that you can use for a preliminary inspection of your data, including figuring out what dimensions it has and what information is contained in your data set.

Call the following functions on your object and describe what each function does, what information you can learn about your data set from executing them, and when these could be helpful. 

* `dim()`, `nrow()`, `ncol()`
* `str()`, `summary()`
* `head()`, `tail()`
* `colnames()`, `rownames()`
* `View()`

`r msmbstyle::question_end()`

Similar to the way we were able to subset vectors we can do the same things with our `data.frames` using rows and columns as our "coordinates" in the format `data_frame[row_index, column_index].

So for example we can extract the first row and colum from our `catch` object as

```{r}

catch[1, 1]

```

`r msmbstyle::question(label = "ques:df-4", text = "How would you extract the 5th column from the third row?")`

You can select multiple rows or columns by specifying them using a vector.

```{r}

catch[c(1, 20, 40), c(2, 5)]

```

You can also select a set of adjacent rows (columns) using `:` as so

```{r}

catch[500:505, 2:5]

```

You can exclude indices using `-`

```{r}

catch[1:5, -1]

```

You can select all columns of a given row by leaving the column index blank; for example if we want to extract the first row.

```{r}

catch[1, ]

```

`r msmbstyle::question(label = "ques:df-5", text = "How would you extract the entire 5th column?")`

`r msmbstyle::solution_begin()`

Here are two options using indices:

```{r}

catch[, 1]

catch[1]

```

Instead of using indices you can also call their column names directly - both of these options will return a `data.frame`.

```{r}

catch["Site"]

catch[, "Site"]

```

You can also return them as a vector using square brackets `[[]]` or `$`.

```{r}

catch[["Site"]]

catch$Site

```

`r msmbstyle::solution_end()`

Using indices might seem a little bit clunky, e.g. you have to know which column and row is which by position but it has its practical applications and is computationally very fast. For most of our data wrangling we will be using functions from the tidyverse packages `dplyr` and `tidyr` which work a little bit more intuitively.


## Write `data.frame` to file

Frequently, we will process raw data sets and then need to write intermediate or final results to file, for example to share them with collaborators. Here, the `readr` packages comes in handy. 


`r msmbstyle::question(label = "ques:df-6", text = "Create a subset of our data set consisting of the first 100 rows and containing information on the species, Sex and forklength. Then use the help page for the function write_delim() to figure out how to write out a tab-delimited file.")`

<!--chapter:end:06_DataFrames.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
    includes:
     in_header: header.html
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Welcome to the tidyverse

**Learning Objectives**

After completing this tutorial you should

* be able to read a tab-delimited file into R.
* use `filter()` and `select()` to include/exclude rows and columns from a data frame.
* use `distinct()` to select only unique/distinct rows from a data frame.
* use `separate()` and `unite()` to separate content from a column across multiple columns and vice versa.
* use `mutate()` to add new columns.
* use `group_by()` to group rows.
* use `summarize()` to calculate summary statistics.

You should have a `DataWrangling.Rmd` in your project directory. Use that file to work through this tutorial - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. Remember to use comments to describe your code; at minimum you should have one comment per code set^[You should do this even if you are adding code yourself or if you are using code from our manual, even if it isn't commented in the manual ... you may of course add as many comments as you need to be able to recall what you did].

As always, let's start by loading our packages needed for this analysis.

```{r}

# load libraries (make sure you have installed them first)
library(knitr)
library(tidyverse)

```

## Data Wrangling

Now that we have a data frame to play with, let's explore some data wrangling options using `dplyr`. The `d` stands for `data` and the `plyr` stands for `plyers` - this package is designed to manipulate data frames^[This is also known as `data wrangling`, manipulating is not a negative thing in this case it means you can clean up and format the data in appropriate way to fit the questions you are asking and allow to to display the information in a helpful way; you will also see this described as data transformation.]. We are going to focus on the central actions (`verbs`) that will allow you to manipulate the data frame.

We will compare using excel vs R (or another coding language) down the line but here are some of the main advantages to using code -

* You aren't manipulating the raw data set - if you make a mistake or accidentally overwrite something you haven't made any permanent damage.
* You can manipulate data sets too large to easily handle in Excel.
* If you update your data set or have a second identically formatted data set you just have to re-run the code.

Be sure to record all the steps (code chunks) in your Rmarkdown - both the examples given here and the applications you will be asked to make. You can copy and paste, but you will find that writing out the code will help you get more used to syntax, how autocomplete etc. works. Make use of the fact that you can add headers, text etc. + code chunks. Be sure to comment your code. 

These are central concepts that you will use and reuse throughout the semester so you will likely want to refer back to this document. A good way to create a "cheatsheet" would be to for example for each function write a short description of what it does in general (refer to this manual) before each code chunk, then make your comment in the code specific to your example. Similarly use normal text to refer to the question numbers in this manual as you work through the problem sets.


## Selecting and organizing columns

Let's start by loading our data set.

```{r}

# read catch data
catch <- read_delim("data/longline_catchdata.txt", delim = "\t")

```


```{marginfigure}

We will make heavy use of the magrittr pipe `%>%` this allows you to link commands together; think of it as "and now do this".

Because we are mostly interested in what the individual functions (verbs) do we will not always assign a new object, but just having it print to the console/below the code chunk we will be able to immediately assess the affect. By piping our function to `head()` it will print just the first 6 lines.

```

The function `select()` is used to select a subset of columns from a data set.

For example, you can select just the Site and Species columns.

```{r}

catch %>%
  select(Site, Species) %>%
  head()

```

`r msmbstyle::question(label = "ques:tidy-1", text = "How would you select just Day, Month, and Year columns?")`

You PCL column doesn't contain any information (all `NA`s) - you can also eliminate individual columns by name.

```{r}

catch %>%
  select(-PCL)

```

`r msmbstyle::question(label = "ques:tidy-2", text = "How would you eliminate hook size from the data set?")`

You can also eliminate multiple columns by name, for example you would remove Day, Month and Year like this:

```{r}

catch %>%
  select(-Day, -Month, -Year)

```

If you want to re-arrange columns in your data frame, you would also use `select()`.

```{r}

catch %>%
  select(FL, Sex, Day)

```

If you wanted to move a set of columns to the front, but not have to type in all the other column names you can use `everything()`.

```{r}

catch %>%
  select(Day, Month, Year, everything())

```

There you go. Simple as that.

## Separate & uniting columns

Occasionally you will find that you want to combine the contents of two columns into a single column (e.g. first name, last name) or at other times you may want to separate the contents of a column over multiple columns (e.g. dates). 

For example, you may have noticed that the Species is entered as `genus_species` - what if you wanted to have two separate columns with that information? The function `separate()` is a function that will split the contents from one column across two or more columns. 

To do this you need to specify the new column names (`into = c("column1", "column2")`), and what pattern should be used to determine where the content should be split (`sep = "pattern"`).

```{r}

catch %>%
  separate(Species, into = c("species", "genus"), sep = "_", remove = FALSE)

```

`r msmbstyle::question_begin(label = "ques:tidy-3")`

In some cases, there might not be a distinct pattern that you can use to identify where to split the column content. In this case it may be more helpful to use the position (e.g. "split at "third character from the left") Look up the `separate()` function in the help and determine how you could split the Year column so you get two new columns by splitting off the last two digits (i.e. 2021 would be 20 and 21). Then eliminate the column containing the first two digits.

`r msmbstyle::question_end()`

To do the opposite action you would use `unite()`. For example, if we wanted to create a column called date that had the day, month, and year of each sampling trip separated by an `_`.

```{r}

catch %>%
  unite(Date, Day, Month, Year, sep = "_", remove = FALSE)

```

`r msmbstyle::question_begin(label = "ques:tidy-4")`

Create a data set with the following columns in this sequence

* Set_ID (combining day, month, year, and set)
* Genus
* Species
* FL
* STL

`r msmbstyle::question_end()`


## Filtering (subsetting) rows

Until you want to visualize a table how the rows are arranged is not really important. However, when generating reports for example you might want values to be listed in a specific way. This can be done using the function `arrange()`.

For example, if we wanted to arrange our dataframe based on the `Observed_Stage` column we could do the following:

```{r}

catch %>%
  arrange(Observed_Stage)

```

`r msmbstyle::question(label = "ques:tidy-8", text = "How would sort your table by Sex?")`

Characters are sorted alphabetically, numeric columns from smallest to largest value. If you want to order your values from largest to smallest, you can specify that using `desc()`

```{r}

catch %>%
  arrange(desc(FL))

```

The function `filter()` is used to subset a data frame by row. 

We do this using `regular expressions` and the `boolean operators` we previously encounter to describe the content of sets of rows.

For example, we might a `data.frame` with only Gafftop sail catfish (*Bagre marinus*).

```{r}

catch %>%
  filter(Species == "Bagre_marinus")

```

`r msmbstyle::question(label = "ques:tidy-9", text = "How would you select only rows containing Scalloped Hammerheads (*Sphyrna lewini*)?")`^[Note: you need to use `==` (not `=` if you are looking for an exact match).]

If want all rows but Gafftop sailfish you can use a `!` to say "not that".

```{r}

catch %>%
  filter(!Species == "Bagre_marinus")

```

`r msmbstyle::question(label = "ques:tidy-10", text = "How would you create a dataframe without Scalloped Hammerheads (*Sphyrna lewini*) entries?")`

Sometimes you might want to select rows that match one of a set of values^[Recall, the function `c()` (concatenate) creates a vector].

```{r}

catch %>%
  filter(Species %in% c("Sciades_felis", "Bagre_marinus", "Synodus_foetens"))


```

Again, if you wanted everything but rows containing those values you would preface it with a `!`.

```{r}

catch %>%
  filter(!Species %in% c("Sciades_felis", "Bagre_marinus", "Synodus_foetens"))

```

`r msmbstyle::question(label = "ques:tidy-11", text = "How would you subset a data frame to contain only entries for different species in the genus Carcharhinus aka the sharky-sharks? There are four species in the data set - Carcharhinus_leucas, Carcharhinus_brevipinna, Carcharhinus_limbatus, Carcharhinus_porosus.")`

For numbers you likely aren't just searching for exact matches, you also want to be able to set threshold values and select everything above or below. For example, you can select all rows with values greater than a certain value using `>`.

```{r}

catch %>%
  filter(FL > 440)

```

`r msmbstyle::question(label = "ques:tidy-12", text = "Create a data frame containing only entries with a forklength < 300.")`

You can also combine expressions - for example to select only scalloped hammerheads that are also smaller that 300 cm you would use

```{r}

catch %>%
  filter(Species == "Sphyrna_lewini" & FL < 300)

```

`r msmbstyle::question(label = "ques:tidy-13", text = "How would you subset a data frame to contain only entries for different species in the genus Carcharhinus that have a forklength larger than 500 cm?")`

Instead of entries having to fulfill two (or more) requirements, you might have a situation where you only want to retain rows that fulfill one or another argument. For example, if you wanted all entries that are either gafftops or a fork length smaller than 300 cm you could use the following code^[When you are combining arguments you use `&` to say "it must fulfill this **AND** this" or `|` (called the pipe) to say "it must fulfill this **OR** that.]:

```{r}

catch %>%
  filter(Species == "Sphyrna_lewini" | FL < 300)

```

`r msmbstyle::question(label = "ques:tidy-14", text = "How would you select fish that have a forklength either smaller than 200 cm or larger than 300cm?")`

The tidyverse has a central concept call "split-apply-combine", which means that occasionally we want to group entries in a dataframe (split), do some sort of manipulation (apply), but end up with a single data frame (combine).


```{marginfigure}

The function `max()` can be used to get the maximum value for a vector of numbers. In this case, the vector we are looking at is the `FL` column of the `catch` dataframe.

```

For example, we might want to retain the entry with the longest fork length for each species. To do this we can use the `group_by()` function, then the `filter()` function.

```{r}

catch %>%
  group_by(Species) %>%
  filter(FL == max(FL)) %>%
  ungroup()

```

This is also an example of how we can use the pipe (`%>%`) to string a bunch of commands, in this example we are saying "take the object catch, and then group rows by Species and then for each group retain only the maximum forklength value for that group and then ungroup them again."

`r msmbstyle::question(label = "ques:tidy-15", text = "How would you group rows by Species and then retain the individual caught on the largest hook size for each species?")`

By doing that, you retained only one observation per species. That is actually a common process you might need to perform during exploratory analysis and can be achieved in a straightforward way using the function `distinct()`. For example, if we wanted to know all the years that had been sampled in we could use 

```{r}

catch %>%
  distinct(Year)

```
Notice how that dropped all the other columns. You can switch that off usiing `.keep_all = FALSE`.

`r msmbstyle::question(label = "ques:tidy-16", text = "How would you produce a table with only one representative per species?")`

You can also combine columns. For example if we wanted to determine the individual sets of the data set we could use 

```{r}

catch %>%
  distinct(Day, Month, Year, Set)

```

`r msmbstyle::question(label = "ques:tidy-17", text = "How would you produce a table showing all the species caught per station using distinct(), presented as arranged alphabetically by Site and Species within Site?")`


## Adding new variables

Turns out selecting columns and filtering based on content in rows is pretty straightforward. But frequently when we are processing our raw data sets we end up wanting to compute additional metrics or use the existing raw data to create new categories.

The function `mutate()` can be used to create new columns - frequently based on columns already existing in the data frame. This is a very powerful function with endless possibilities, but we are going to stick to some of the basics for now^[Rest assured if your answer is "Oh, could I ..." the answer is "Yes".].

Let's say you wanted create a column that contained the difference between the fork length and the stretch total length:

```{r}

catch %>%
  mutate(difference = STL - FL)

```

You should now have a column called `difference` at the end of the data frame^[Instead of `-` you can of course use `+` to add , `*` to multiple, and `/` to divide values when creating a new column.].

`r msmbstyle::question(label = "ques:tidy-18", text = "How would you create a new column called ratio, that is the ratio of the fork to stretch total length?")`

You can also create a column that contains a logical value (`TRUE`/`FALSE`). For example we might need a column that indicates if the Sex is unknown.

```{r}

catch %>%
  mutate(unknown_sex = Sex == "U")

```

You should know have a column called `unknown_sex` where if the animal that was caught was not sexed contains the value `TRUE`, if it was identified as male or female it would say `FALSE`.

`r msmbstyle::question(label = "ques:tidy-19", text = "How would you create a new column called post_2017 that is TRUE if fish were caught after 2017?")`

```{marginfigure}

Note: For that last one, a "conditional mutate" using an `ifelse` statement (if this then do that, else do that) could have come in handy. Another option is `case_when()` which allows you to create multiple sets of conditions as opposed to `ifelse` which sets up a `TRUE`/`FALSE` dichotomy (file this information away for "maybe useful later").

```

Many problems in data science require you to split your data set into subsets according to some grouping variable, apply a function, and then combine the results. `dplyr` is designed to make this straightforward; you have already sen an example of this while you were learning about `filter()`. Similarly, you can combine `mutate()` with `group_by()`.

```{marginfigure}

The function `mean()` will calculate the mean value of a vector of numbers, the argument `na.rm=TRUE` tells the function to ignore any `NA`-values in the data set.

```

For example, let's say you wanted to create a column that is the difference between the fork length of an individual and the mean fork length of *that species*.

```{r}

catch %>%
  group_by(Species) %>%
  mutate(diff_mean = FL-mean(FL, na.rm = TRUE))

```

`r msmbstyle::question(label = "ques:tidy-20", text = "How would you create a new column called that contains the difference between the fork length of an individual and the mean fork length of that species for each month?")`


## Create new `data.frame` based on another

Not infrequently we are more interested in summary stats of a data set rather than all the raw data - `Tidyverse` got you covered with the function `summarize()`.

For example, we might want to calculate the mean and standard deviation of the measured fork length.

```{r}

catch %>%
  summarize(mean_FL = mean(FL, na.rm = TRUE),
            sd_FL = sd(FL, na.rm = TRUE))

```

`r msmbstyle::question(label = "ques:tidy-21", text = "How could you use summarize to get the maximum forklength?")`^[Hint: Remember earlier we used the function max() to get a maximum value.]

That's cool but really we could have also just used 

```{r}

mean(catch$FL, na.rm = TRUE)

```
to get that information.

`summarize` becomes especially powerful once we leverage `group_by()` to start calculating summary stats for entries grouped by a grouping variable.

For example we can calculate summary stats by species:

```{r echo=FALSE}

catch %>%
  group_by(Species) %>%
  summarize(mean_FL = mean(FL, na.rm = TRUE),
           median_FL = median(FL, na.rm = TRUE),
           max_FL = max(FL, na.rm = TRUE),
           min_FL = min(FL, na.rm = TRUE),
           sd_FL = sd(FL, na.rm = TRUE)) %>%
  ungroup()

```

`r msmbstyle::question(label = "ques:tidy-22", text = "If you look closely you should see that you are getting a few NA, NaN, -Inf, and Inf values - any guesses why? You might want to pull up the catch data frame in the view panel to see what is going on with those species.")`

`r msmbstyle::question(label = "ques:tidy-23", text = "How could you use summarize calculate a range of summary stats for the stretch total length for individuals grouped by sex?")`

So far, we have been manipulating our data frame using code and printing it directly to the console (and our rmarkdown). This can be useful for example to generate tables for reports but frequently we want to create a new object that has been manipulated according to our code and then we will further process, visualize, or analyze that dataframe down the line.

```{r}

summary <- catch %>%
  group_by(Species) %>%
  summarize(mean_FL = mean(FL, na.rm = TRUE),
           median_FL = median(FL, na.rm = TRUE),
           max_FL = max(FL, na.rm = TRUE),
           min_FL = min(FL, na.rm = TRUE),
           sd_FL = sd(FL, na.rm = TRUE)) %>%
  ungroup()

```

When you execute this code, you'll notice that the code (and probably a warning) is printed to the console but there is no output. Instead, if you look at the environment in the top right corner you should now see a new object called `summary`. Per usual, you can pull that up in the Editor/View pane (top left) using either `View(summary)` in the console or by clicking on the object in the environment.

You will be presenting results in reports over the course of the semester, when you `knit` an `Rmarkdown` file you will get tables formatted in a standard way according to defaults in the resulting `html` file. If you want finer control over the output, you can use the `kable()` function. This will allow you to further format the table, for example, you may specify the number of digits printed using the argument `digits = ` and add a caption using `caption = "caption"`.

```{r}

kable(
  summary,
  digits = 1,
  caption = "Summary statistics of forklength for each species."
)

```


## Combining verbs

We've already combined most of our `dplyr` verbs with `group_by()`. When you are wrangling data you will find that making use of the pipe (`%>%`) to combine `select()`, `filter()`, `mutate()`, and `summarize()` as a series of commands will be necessary to get your data set in the correct format and further process it.

`r msmbstyle::question_begin(label = "ques:tidy-24")`

Copy this code into your `Rmarkdown` file and execute the code chunk. Then describe what each line is doing to manipulate the data frame.

```{r}

catch %>%
  select(-PCL, -Hook_Size) %>%
  separate(Species, into = c("genus", "species"), remove = TRUE) %>%
  unite(Date, Day, Month, Year) %>%
  filter(genus == "Carcharhinus" & Sex %in% c("F", "M")) %>%
  group_by(Site, genus, species, Sex) %>%
  filter(FL == max(FL)) %>%
  arrange(species)

```

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:tidy-25")`

Generate the code that will manipulate the data frame as follows^[some bullet points may require more than one line of code; you do not have to perform the steps in the sequence presented, play around a little bit to see how to code this more efficiently]:

Challenge 1:

* order columns so Day, Month, Year, Set are at the beginning.
* retain all male individuals in the genus Carcharhinus.
* get rid of columns containing information on observed stage, precaudal length, and hook size

Challenge 2:

* create a new variable called Set_ID consisting of Day, Month, Year, and Set number.
* determine the number of individuals per species per set^[There is a function called `n()` that allows us to count rows fulfilling a specific condition].

Challenge 3:

* remove all gafftops
* calculate mean forklength for each species by sex and month of the year.

format each data frame in a separate code chunk.

`r msmbstyle::question_end()`


## Producing tidy data sets

The last set up functions that we need to get comfortable with allow us to create tidy data sets. 

`r msmbstyle::question(label = "ques:tidy-26", text = "List the three characteristics of a tidy data set. Explain why a tidy data set is sometimes also describe as a long data set.")`

`r msmbstyle::question(label = "ques:tidy-27", text = "Take a look at our data set and argue whether or not it is a tidy data set. The easiest way to do this is to determine if it fullfills all the characteristics.")`

Let's quickly reformat our catch data as follows

```{r}

catch_length <- catch %>%
  unite(SetID, Year, Month, Day, Set, sep = "_") %>%
  select(SetID, Site, Species, Sex, PCL, FL, STL)

head(catch_length)

```

We can turn this into a tidy data set using `pivot_longer()`. To do that we have to identify columns that will be used as the key (`cols = `) and then name the column that will hold those values (`names_to()`) and the column that will hold the value (`values_to()`).

So in this case `Measurement` would be the key. This column will let us know whether an observation (row) is pre-caudal length, fork length, or stretch total length.

We will designate another column `Length` to hold the values for each measurement.

We can identify the columns that need to be gathered either by name or since we have re-arranged our dataframe so they are the last columns but column number.

```{r}

tidy_length <- catch_length %>%
  pivot_longer(names_to = "Measurement", values_to = "Length", cols = 5:7)

```

`r msmbstyle::question(label = "ques:tidy-28", text = "Briefly outline advantages to using tidy data sets.")`

With this data set it would be straightforward for us to e.g. calculate mean values for each length measurement by species.

```{r}

tidy_length %>%
  group_by(Species, Measurement) %>%
  summarize(mean = mean(Length, na.rm = TRUE))

```

Despite all the advantages of tidy data sets you can see from the table above that frequently when we are presenting results in a table it may be advantageous in terms of layout to have a non-tidy format. 

This can be done using `pivot_wider()` which essentially works like `pivot_longer()` but in reverse. You designate which column is the key (`names_from = `), i.e. these will become the column names in the new table.

Then you need to identify which column in your current data frame contains the values that should be filled out/spread into the columns that will be generated from your key (`values_from =`).

Since we don't have values for precaudal length, we probably want to use `filter()` to remove those rows first.

```{marginfigure}

More notes on naming things ... recall, that we said that filenames should not contain spaces or special characters? We set similar rules for naming objects. Well, column names is a similar conundrum. Including spaces or species characters as a column name creates problems when we are using functions like `select()` to subset by column name or `mutate()` to create new columns based on exisiting columns. Similarly, if the column name is a number you will have problems. If you do have unconvential column names you can rename them using `rename()` or you can use backticks and either side of the name to indicate that it is a column name.

```

```{r}

tidy_length %>%
  filter(!Measurement == "PCL") %>%
  group_by(Species, Measurement) %>%
  summarize(mean = mean(Length, na.rm = TRUE)) %>%
  pivot_wider(names_from = "Measurement", values_from = "mean")

```

`r msmbstyle::question(label = "ques:tidy-29", text = "Calculate the number of individuals per species caught per month in 2018 and present that data in a wide formate to make it easy to compare the number of species (species) per month (columns). As a bonus create an additional column with total catch of that species for 2018.")`


<!--chapter:end:07_DataWrangling.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Catch data analysis

**Learning Objectives**

After completing this activity you should be able to

* understand the differences between absolute abundance, relative abundance, and catch per unit effort (CPUE) and how to use them appropriately.
* `bind_rows()`
* `left_join()`
* understand what a "conditional mutate" is.

Now that you've got your basic data wrangling skills down, let's see what we can learn from the long-lining survey data. Remember, the overarching goal of the study was to answer these questions:

1. How does the composition of elasmobranch communities compare across sites?
2. How does the catch-per-unit-effort (CPUE) per species and life history compare across sites?
3. What do the sex ratios look like?
4. What environmental predictors can we use to predict presence of elasmobranchs?

We are going to stick to the first two questions to practice our data wrangling skills (and possibly pick up a few new ones ...)

You should have a `NurseryAnalysis.Rmd` in your project directory. Use that file to work through this tutorial - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. Remember to use comments to describe your code; at minimum you should have one comment per code set^[You should do this even if you are adding code yourself or if you are using code from our manual, even if it isn't commented in the manual ... you may of course add as many comments as you need to be able to recall what you did].

As always, let's start by loading our packages needed for this analysis.

We are missing one package that we will need to install the first time, it contains a specialized function that we will need down the line.

```{r}

# install packages
# install.packages("FSA")

```

Now let's load our packages.

```{r}

# load libraries
library(knitr)
library(tidyverse)
library(FSA)

```


## Composition of elasmobranch communities compare across sites

Let's start by reading in the data set we will use for this analysis.

```{r}

elasmos <- read_delim("data/longline_elasmobranchs.txt", delim = "\t")

```



`r msmbstyle::question(label = "ques:shn-1", text = "Produce a table that contains the number of times a species was caught at each site and overall during the long-lining survey and give a brief description of the pattern(s) you see. Briefly, compare the list of species that were caught to the species identified in the longterm TWPD gill net monitoring program.")`

Your table should look something like this. ^[Pro tip: you can use `replace(is.na(.), 0)` to replace `NA` values in all columns with a `0`.]

```{r echo=FALSE}

kable(
  elasmos %>%
    group_by(Species, Site) %>%
    count() %>%
    pivot_wider(names_from = Site, values_from = n) %>%
    replace(is.na(.), 0) %>%
    mutate(Total = Aransas_Bay + Corpus_Christi_Bay + Redfish_Bay),
  caption = "Number of individuals per caught per site and overall across all sites and years."
)

```

We are not only interested in which species we can observe at each site, we also want to know what at what life stages different species are using the estuaries. Typically, we can classify sharks as young-of-the-year (YOY), juveniles (JUV), or mature (MAT). There are ways to observe this in the field, for example YOY can be identified using their umbilical scar and in male sharks whether or not the claspers are calcified is an indication of maturity. 

`r msmbstyle::question(label = "ques:shn-2", text = "Determine how many individuals have information on their life history stage.")`

Another way to determine the life history stage is to used previously information on length-at-maturity and how quickly YOY grow during their first year of life. This information is species-specific and has been determined for various species using life-history studies that rely on data sets that contain information on size, level of maturity and age^[Sharks can be aged using their vertebrae similar to how we can use growth rings on trees to age them.].

`r msmbstyle::question_begin(label = "ques:missed")`

For example, Carlson and Baremore 2005 [@Carlson2005] determined the following length/history stage relationships for spinner sharks (*C. brevipinna*)

* YOY
  + females < 844mm
  + males < 812mm
* Juveniles
  + females 844 - 1360mm
  + males 812 - 1380mm
* mature (adults)
  + females > 1360 mm
  + males > 1380 mm


While Neer et al. 2005 published these details for bull sharks (*C. leucas*)

* YOY
  + females < 700mm
  + males < 700mm
* Juveniles
  + females 700 - 2250mm
  + males 700 - 2100mm
* mature (adults)
  + females > 2250mm
  + males > 2100mm
  
First, conceptually describe how you could add this information to your data sheet in excel as a new column called `Estimated_Stage`.

Now, let's consider how we could use our data wrangling skills to add a new column `Estimated_Stage` that contains life history stage based on length estimates. Let's first work this out for the two species above to keep it simple. 

When confronted with a more complex problems like this it can be helpful to first walk through the individual steps necessary^[Some people find it helpful to write things out in 'pseudo-code' first and then work out what the code needs to look like for the specific language they are working in].

Briefly describe what you think our approach should look like - even if you don't know the functions you need to achieve this.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

There are two approaches we can take.

The first one would involve sub-setting your `data.frame` using `filter()` to contain only individuals that fulfill the conditions of specific length ranges that fit the ranges above for each life history stage and the add a new column with the correctly assigned life history stage^[Remember, you can use `&` and `|` to combine two conditions].

```{r}

# C. brevipinna, Carlson & Baremore 2005

C.brevipinna_YOY <- filter(elasmos, Species=="Carcharhinus_brevipinna" & Sex=="M" & FL<=812 | Species=="Carcharhinus_brevipinna" & Sex=="F" & FL<=844 | Species=="Carcharhinus_brevipinna" & Sex=="U" & FL<=844) %>%
  mutate(Estimated_Stage="YOY")

C.brevipinna_JUV <- filter(elasmos, Species=="Carcharhinus_brevipinna" & Sex=="M" & FL>812 & FL<=1380 | Species=="Carcharhinus_brevipinna" & Sex=="F" & FL>844 & FL<=1360 | Species=="Carcharhinus_brevipinna" & Sex=="U" & FL>844 & FL<=1360) %>%
  mutate(Estimated_Stage="JUV")

C.brevipinna_MAT <- filter(elasmos, Species=="Carcharhinus_brevipinna" & Sex=="M" & FL>1380 | Species=="Carcharhinus_brevipinna" & Sex=="F" & FL>1360 | Species=="Carcharhinus_brevipinna" & Sex=="U" & FL>1360) %>%
  mutate(Estimated_Stage="MAT")


# C. leucas, Neer et al. 2005

C.leucas_YOY <- filter(elasmos, Species=="Carcharhinus_leucas" & FL<=700) %>%
  mutate(Estimated_Stage="YOY")

C.leucas_JUV <- filter(elasmos, Species=="Carcharhinus_leucas" & Sex=="M" & FL>700 & FL<=2100 | Species=="Carcharhinus_leucas" & Sex=="F" & FL>700 & FL<=2250 | Species=="Carcharhinus_leucas" & Sex=="U" & FL>700 & FL<=2250) %>%
  mutate(Estimated_Stage="JUV")

C.leucas_MAT <- filter(elasmos, Species=="Carcharhinus_leucas" & Sex=="M" & FL>2100 | Species=="Carcharhinus_leucas" & Sex=="F" & FL>2250 | Species=="Carcharhinus_leucas" & Sex=="U" & FL>2250) %>%
  mutate(Estimated_Stage="MAT")


```

Now you have a bunch of individual `data.frames` that we need to put back together into a single `data.frame`. We can do this using `bind_rows()` which will combine `data.frames` that have the same set of columns.

```{r}

elasmos_stage <- bind_rows(C.brevipinna_YOY, C.brevipinna_JUV, C.brevipinna_MAT,
                           C.leucas_YOY, C.leucas_JUV, C.leucas_MAT)

```

This fits into our general scheme of "split-apply-combine" except that we actually created multiple objects during our "split" stage. Is there a way to do this without creating individual objects?

Indeed, our second option circumvents having to first create subsets of the initial `data.frame` using something called a "conditional mutate".

```{r}

elasmos_stage <- elasmos %>%
  filter(Species %in% c("Carcharhinus_leucas", "Carcharhinus_brevipinna")) %>%
  mutate(Estimated_Stage = case_when(Species == "Carcharhinus_brevipinna" & Sex=="M" & FL<=812 |
                                       Species == "Carcharhinus_brevipinna" & Sex=="F" & FL<=844 |
                                       Species == "Carcharhinus_brevipinna" & Sex=="U" & FL<=844 ~ "YOY",
         Species=="Carcharhinus_brevipinna" & Sex=="M" & FL>812 & FL<=1380 |
           Species=="Carcharhinus_brevipinna" & Sex=="F" & FL>844 & FL<=1360 |
           Species=="Carcharhinus_brevipinna" & Sex=="U" & FL>844 & FL<=1360 ~ "JUV",
         Species=="Carcharhinus_brevipinna" & Sex=="M" & FL>1380 |
           Species=="Carcharhinus_brevipinna" & Sex=="F" & FL>1360 |
           Species=="Carcharhinus_brevipinna" & Sex=="U" & FL>1360 ~ "MAT",
         Species=="Carcharhinus_leucas" & FL<=700 ~ "YOY",
         Species=="Carcharhinus_leucas" & Sex=="M" & FL>700 & FL<=2100 | 
           Species=="Carcharhinus_leucas" & Sex=="F" & FL>700 & FL<=2250 | 
           Species=="Carcharhinus_leucas" & Sex=="U" & FL>700 & FL<=2250 ~ "JUV",
         Species=="Carcharhinus_leucas" & Sex=="M" & FL>2100 | 
           Species=="Carcharhinus_leucas" & Sex=="F" & FL>2250 | 
           Species=="Carcharhinus_leucas" & Sex=="U" & FL>2250 ~ "MAT"))

```

This is of course a fairly complicated conditional mutate as we are generally combining multiple conditions per category. In this case we could also leave out the `|` and instead add a `~ STAGE` to each line depending on our coding preferences.

`r msmbstyle::solution_end()`

Normally, we would have to extend our code to estimate life history stage for all of our sampled individuals but I have done this for you and you can load that file from your `data` folder.

```{r}

elasmos <- read_delim("data/elasmos_complete.txt", delim = "\t")

```

`r msmbstyle::question(label = "ques:shn-3", text = "Use this data set to create a table with the number of individuals per life history stage caught at each site.")`

```{r echo=FALSE}

kable(
  elasmos %>%
    group_by(Species, Site, Estimated_Stage) %>%
    count() %>%
    pivot_wider(names_from = Estimated_Stage, values_from = n) %>%
    replace(is.na(.), 0) %>%
    mutate(Total = YOY + JUV + MAT + UND) %>%
    select(Species, Site, Total, YOY, JUV, MAT, UND) %>%
    arrange(Species),
  caption = "Number of individuals per species caught at each site by life history stage."
)

```

`r msmbstyle::question(label = "ques:shn-4", text = "Briefly describe your results to compare total catch across sites accounting for differences in life history stage.")`

`r msmbstyle::question(label = "ques:shnn-4", text = "Subset your data to contain only YOY and generate a table to investigate whether they were caught across all years sampling occured. Summarize your results in 2-3 sentences.")`

```{r echo=FALSE}

kable(
  elasmos %>%
    filter(Estimated_Stage == "YOY") %>%
    group_by(Site, Year, Species) %>%
    count() %>%
    arrange(Year) %>%
    pivot_wider(names_from = Year, values_from = n) %>%
    replace(is.na(.), 0) %>%
    arrange(Site, Species)
)

```


## Comparison of CPUE per species across sites

`r msmbstyle::question(label = "ques:shn-5", text = "Consider disadvantages of using absolute counts of occurence to compare composition across sites. What measure could you use instead of total catch to fix this issue?")`

Catch-per-unit-effort (CPUE) is a indirect measure of abundance. Essentially, it is a way to measure relative abundance and be able to account for differences in sampling effort - the key is defining how you will measure "effort".

`r msmbstyle::question(label = "ques:shn-6", text = "Briefly discuss what measures we could use to determine effort.")`

We are going to calculate effort as "hook hours". To do this we need to know how many hooks were on the line per set^[A set means that baited hooks on leaders (individual lines) where attached to the main line and that main line was then "set" in the water for a given period of time before pulling it back in and determining which fish were caught on hooks.] and how long the entire line was in the water per set (this is called soak time), then we can easily calculate hook hours of each set as the number of hooks multiplied by the soak time. And then we can divide the number of e.g. sharks caught on a set ("catch) by hook hours ("effort") to calculate CPUE.

Your `data` folder contains as tab-delimited file with set meta-data. This includes information that describes the set itself including date of the set, site, soak time, and location and also parameters describing the conditions of the set such as temperature, salinity, depth, and dissolved oxygen.

Let's read in the data set.

```{r}

set_meta <- read_delim("data/set_data.txt", delim = "\t")

```

`r msmbstyle::question(label = "ques:shn-7", text = "Take a quick look at the data set to determine what columns are included and what information we can learn about each individual set. How can you amend the data set to include hook hours?")`

`r msmbstyle::solution_begin()`

Correct, a simple `mutate()` will do the trick.

```{r}

set_meta <- set_meta %>%
  mutate(Hook_Hours = Hooks * Soak_Time)

```

`r msmbstyle::solution_end()`

Next we need to count the number of sharks caught per set.

`r msmbstyle::question(label = "ques:shn-8", text = "If we look at our elasmo data.frame you will notice that we have a column called Set but that number indicates the nth set of a give sample day. How can you add a column called Set_ID that consists of the date and the set number?")`

`r msmbstyle::solution_begin()`

```{r}

elasmos <- elasmos %>%
  unite(Set_ID, Year, Month, Day, Set, sep = "_", remove = FALSE) %>%
  arrange(Set_ID)

```

`r msmbstyle::solution_end()`

`r msmbstyle::question(label = "ques:shn-9", text = "Now create a new object called elasmos_set that contains the number of sharks caught per set.")`

`r msmbstyle::solution_begin()`

```{r}

elasmos_set <- elasmos %>%
  count(Set_ID)

```

`r msmbstyle::solution_end()`

Now we have two `data.frame`s one contains the information on how many sharks were caught per set and a second one that contains information about the set, including hook hours. This means that our next step will need to be to combine these two data sets.

`r msmbstyle::question_begin(label = "ques:shn-10")`

Earlier we learned about `bind_rows()` which allows us to combine two `data.frame`s that contain identical columns, i.e. row-wise. There is an equivalent function called `bind_columns()` which allows us to combine `data.frame`s column-wise. 

Consider what the problem would be in using `bind_columns()` to combine these two data sets.

`r msmbstyle::question_end()`

Having multiple tables containing data pertaining to the same question is referred to as **relational data** - we are interested in how the contents of a pair of table related to each other, not just the individual data sets. Combining two tables is called a **join**. In this case the type of join we want to execute is called a **mutating join** which means we can add new variables from one `data.frame` (`set_meta`) to matching observations in another (`elasmos_set`).

In order to do that we need to have one column (the **key**) that way the function can match observations in one `data.frame` by that **key** and then copy the matching observations in the columns from the second `data.frame` across. 

When performing a **join**, new columns are added to the right. We will use the function `full_join()` which means that all the rows from the left and right `data.frame` will be retained - when we used `count()` that excluded sets where no sharks were caught, by using a `full_join()` we can add those back in.

We currently do not have a matching column between the two data sets, so our first step will be to add a new column called `Set_ID` to our `set_meta` `data.frame`, then we can use `full_join()` to join the two tables. The argument `by` can be used to specify the column to use as the key. For our example here we have a column with the same name - in general, the function is "smart" enough to identify shared columns and so you do not necessarily have to specify it^[You can pull up the help page using `?full_join` to learn how to join tables that have multiple columns in common or that might have a column in common though it is named differently between the two tables.]. 

```{r}

# add set id column
set_meta <- set_meta %>%
  unite(Set_ID, Year, Month, Day, Set, sep = "_", remove = FALSE)

# join data sets
elasmos_set <- elasmos_set %>%
  full_join(set_meta) %>%
  replace_na(list(n = 0))

```

Note the notation `elasmo_set <- full_join(elasmos_set, set_meta, by = "Set_ID")` will produce the same result.

Now we can calculate CPUE for sharks per site.

```{r}

elasmos_set <- elasmos_set %>%
  mutate(CPUE = n/Hook_Hours)

```

And from that we can easily calculate mean and standard deviation CPUE of catching sharks by site.

```{r echo=FALSE}

kable(
  elasmos_set %>%
    group_by(Site) %>%
    summarize(mean_CPUE = mean(CPUE),
            std_CPUE = sd(CPUE)),
  caption = "mean +/- sd CPUE"
)

```

We are going to perform a Kruskal-Wallis rank sum test to determine if there is significant heterogeneity among sites^[You are probably more familiar with the framework of using an ANOVA to test for significant heterogeneity and pairwise t-tests to test for equality of means of a set of values. KW is similar but is a non-parametric approach and does not make assumptions about the distribution of values.].

```{r}

# KW to test for significant heterogeneity
kruskal.test(CPUE ~ Site, data = elasmos_set)

```

And we will follow that using a Dunn's test for pairwise comparisons.

```{r}

# post-hoc Dunn test
dunnTest(CPUE ~ Site, data = elasmos_set, method = "bh")

```

`r msmbstyle::question(label = "ques:shn-11", text = "Briefly describe your results and discuss what this result could mean for our overarching question of identifying shark nurseries.")`

Of course, we are interested how CPUE compares across species and sites.

`r msmbstyle::question(label = "ques:shn-12", text = "Choose one species and calculate the CPUE per set. For convenience convert CPUE to effort per 1000 hook hours and then calculate the mean CPUE per 1000 hooks per site for that species.")`

`r msmbstyle::solution_begin()`

This is what that could look like for a single species.

```{r echo=FALSE}

species <- "Carcharhinus_brevipinna"

species_CPUE <- elasmos %>%
  filter(Species == species) %>%
  count(Set_ID) %>%
  full_join(set_meta) %>%
  replace_na(list(n = 0)) %>%
  mutate(CPUE = n/Hook_Hours,
         CPUE_1000 = CPUE * 1000) %>%
  group_by(Site) %>%
  summarize(mean_CPUE = mean(CPUE_1000))

kable(
  species_CPUE %>%
    pivot_wider(names_from = Site, values_from = mean_CPUE) %>%
    replace(is.na(.), 0),
  caption = "Catch per unit effort (1000 hook hours) for each site.",
  digits = 2
)

```

We actually want to have this information for all species, rather than create individual `data.frame`s for each species and then combine those using `bind_rows()`, I will show you a more efficient way of coding this using a `for loop`^[There are a few other ways of during this but for now let's just stick to the fact that this is an efficient way to get results.].

```{r}

# create empty list
species_CPUE <- list()

# calculate hook hours for each species per site
for(species in unique(elasmos$Species)){
  
  species_CPUE[[species]] <- elasmos %>%
    filter(Species == species) %>%
    count(Set_ID) %>%
    full_join(set_meta) %>%
    replace_na(list(n = 0)) %>%
    mutate(CPUE = n/Hook_Hours,
           CPUE_1000 = CPUE * 1000)

}

# combine data frames in list into single 
CPUE <- bind_rows(species_CPUE, .id = "Species")

```

Similarly, we would want to run KW tests to determine if there are significant differences among sites. 

```{r}

# create empty dataframe for results
results <- setNames(data.frame(matrix(ncol = 2, nrow = 0)), 
                    c("Species", "pvalue")) %>%
  mutate(Species = as.character(Species),
         pvalue = as.numeric(pvalue))

for(species in unique(CPUE$Species)){
  
  # filter CPUE per species
  tmp <- CPUE %>%
    filter(Species == species)
  
  # KW to test for significant heterogeneity
  KW <- kruskal.test(CPUE ~ Site, data = tmp)
  
  # extract p-value
  df <- data.frame("Species" = species,
                   "pvalue" = as.numeric(KW$p.value))
  
  results <- bind_rows(results, df)

}

```

Let's calculate mean CPUE per species and site, turn that into a wide table for easier comparison and add the p-values.

```{r}

CPUE_sign <- CPUE %>%
    group_by(Species, Site) %>%
    summarize(mean_CPUE = mean(CPUE_1000)) %>%
    pivot_wider(names_from = Site, values_from = mean_CPUE) %>%
    left_join(results) %>%
    arrange(Species)

```

```{r echo=FALSE}

kable(
  CPUE_sign,
  digits = 2,
  caption = "Catch per unit effort (per 1000 hook hours) for each species by site."
)

```

`r msmbstyle::solution_end()`

`r msmbstyle::question(label = "ques:shn-13", text = "Briefly describe the results comparing CPUE per species and site and relate that to our overarching question of identifying shark nurseries; you may need to run additional pairwise tests (Dunn's test) to determine significant differences.")`


## Comparison of CPUE for different life history stages

Of course, we are not only interested in which species were caught at each site, we also want to know what life history stages those individuals were at when they were caught.

We will use a similar strategy as above to create a data frame with CPUE per site, species, and life history stage and produce a table comparing the means.

```{r echo=FALSE}

# create empty list
species_CPUE <- list()

# calculate hook hours for each species per site
for(species in unique(elasmos$Species)){
  
  for(stage in unique(elasmos$Estimated_Stage)){
    
      species_CPUE[[paste(species, stage, sep = ":")]] <- elasmos %>%
        filter(Species == species & Estimated_Stage == stage) %>%
        count(Set_ID) %>%
        full_join(set_meta) %>%
        replace_na(list(n = 0)) %>%
        mutate(Estimate_Stage = stage, 
               CPUE = n/Hook_Hours,
               CPUE_1000 = CPUE * 1000)

  }
  
}

# combine data frames in list into single 
CPUE <- bind_rows(species_CPUE, .id = "Species_Stage") %>%
  select(Species_Stage, Set_ID, Site, Hooks, Soak_Time, Hook_Hours, CPUE, CPUE_1000) %>%
  separate(Species_Stage, into = c("Species", "Stage"), sep = ":", remove = FALSE)

kable(
  CPUE %>%
    group_by(Species_Stage, Site) %>%
    summarize(mean_CPUE = mean(CPUE_1000)) %>%
    pivot_wider(names_from = Site, values_from = mean_CPUE) %>%
    filter(if_any(c(Aransas_Bay, Corpus_Christi_Bay, Redfish_Bay), ~ . > 0)) %>%
    separate(Species_Stage, into = c("Species", "Stage"), sep = ":") %>%
    filter(!Stage == "UND")
)

```

`r msmbstyle::question(label = "ques:shn-14", text = "Briefly describe the results comparing CPUE per life history stage and site and relate that to our overarching question of identifying shark nurseries; these results are all statistically significant.")`

<!--chapter:end:08_SharkNurseriesAnalysis.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: TRUE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Scientific misconduct and the reproducible crisis

**Learning Objectives**

After completing this set of activities you should be able to

* Define what scientific misconduct is and describe common unethical practices that may or may not fall into that strict definition.
* Define what the ‘reproducibility crisis’ is, common practices that are contributing, and understand how some fields/methodologies are more susceptible.
* Distinguish between different definitions for reproducibility and replicability, and explain how they contribute to the confidence in scientific results being “correct” by the scientific community and public.


After all that data wrangling and learning new functions and data science concepts we are going to take a step back and think a little bit about our workflow and what standards we should set for ourselves and others in terms of how we generatate and analyze data and who we communicate results.

Go ahead and download the [Project Folder](https://drive.google.com/drive/folders/1DJRrb_S-iRGzEoaGzIqMukAeWp9o1mj3?usp=sharing) for this Module. Once you have downloaded it, unzip the project directory into your `BI449` directory. You can open the `Rproj` for this module either by double clicking on it which will launch `Rstudio` or by opening `Rstudio` and then using `File > Open Project` or by clicking on the `Rproject` icon in the top right of your program window and selecting `Open Project`. 

Once you have opened a project you should see the project name in the top right corner^[Pro tip: If you run into issues where an Rmarkdown won't render or file paths aren't working (especially if things were working previously) one of your first steps should be to double check that the correct `Rproj` is loaded.].

You should have a `ReprodCrisis.Rmd` in your project directory. Use that file to work through this tutorial - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. For this module there will not be any coding involved (no code chunks!) instead you should work a little bit on your `Rmarkdown` skills.

In your document try using some of the formatting skills:

```{marginfigure}

You have various resources at your disposal, including the `Rmarkdown` [cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf) you received a handout for and [Chapter 27](https://r4ds.had.co.nz/r-markdown.html) in our Textbook. The easiest way to learn `Rmarkdown` is to try out the syntax and then `knit` your document to see if it turns out as you think it should!

```

* Use different level headers to structure your document
* include something in **bold** and something in *italics*.
* add numbered and unordered bullet points (maybe even try adding multiple levels if you're feeling adventerous).

For these activities we will do all the computing (thinking) with our own brains ... so let's dive straight in!


## Frabrication, falsification, and plagiarism (oh my!)

`r msmbstyle::question_begin(label="ques:RR1-01")`

The National Science Foundation defines **scientific misconduct** in the categories of **fabrication**, **falsification**, and **plagiarism**. Give a brief definition of each term.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR1-02")`

Take the set of descriptions that describe hypothetical^[hypothetical because not referring to a specific case study but these are very realistic scenarios of daily decision making during research.] scenarios of decision making during data generation and analysis. Classify each as either scientific misconduct according to NSF's definition or not.

Identify scenarios that you are not sure of which category they fall into and be ready to discuss with the class.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR1-03")`

Take your scenarios and rank them along a continuum of ethical to unethical. 

Consider these aspects to establish your ranking:

* are some unethical practices worse than others?
* which scenarios do you think are more common than others?
* do you think some are easy to get away with?
* how easy do you think it is to detect if something like this has taken place?
* whose responsibility is it to ensure unethical conduct does not take place during the research process?


`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR1-04")`

After discussion with the class identify at least five major categories of misconduct and unethical behavior and give an example for each. Briefly discuss why for some categories identifying misconduct and unethical conduct is more clear cut while for others it can be difficult to draw a definitive line.

`r msmbstyle::question_end()`


## P-hacking and data dredging

`r msmbstyle::question_begin(label="ques:RR1-05")`

Imagine you are a social scientist interested in how political parties impact the US economy. 

First, develop a hypothesis of whether Democrats or Republicans being in office positively or negatively impacts the US economy. 

Now, use real data going back to 1948 to investigate. To publish your data you would need a statistically significant result. Fortunately you can [hack your way to scientific glory using fivethirtyeight's interactive tool](https://projects.fivethirtyeight.com/p-hacking/). Describe how you were able to confirm your hypothesis by manipulating which group of politicians to include, how you measured economic performance and other options.

Finally, formulate a second opposing theory and see if you can generate a statistically significant result for that^[Pro Tip: Find a p-hacking buddy and test alternate hypotheses and then swap your results!].

`r msmbstyle::question_end()`

Congratulations, you just became a successful p-hacker. The practices of **p-hacking** and **data dredging** have become increasing common in the era of big data.

`r msmbstyle::question_begin(label="ques:RR1-06")`

Briefly describe what the practices of **p-hacking** and **data-dredging** entail.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR1-07")`

Briefly describe what the **reproducibility crisis** is and argue which fields of science you would expect to be more/less heavily impacted and how the increasing availability of large data sets and deployment of complex methods of analysis (including machine learning) have contributed.

`r msmbstyle::question_end()`


## You keep using that word, but ...

`r msmbstyle::question_begin(label="ques:RR1-08")`

**Replicability** and **reproducibility** of studies both generally refer to the practice of validating the results obtained by duplicating them. However, exact definitions of the terms  vary among fields of research. Briefly, argue how you would rank different levels of confidence in the results of a study based on whether it was been repeated with the same results using (combinations of) the same or different teams, the same or different experimental set-ups, and/or the same or different data set.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR1-09")`

The National Science Foundation (NSF) defines "replicability" as "the ability of a researcher to duplicate the results ofa prior study if the same procedures rae followed but new data are collected". 

Goodman et al [-@Goodman2016] propose a framework that defines three categories based on the goals as related to transparency & compete reporting of methods, producing new evidence and drawing the same conclusion. Briefly compare and contrast the categories of **methods reproducibility**, **results reproducibility**, and **inferential reproducibility**.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR1-10")`

Briefly discuss how (lack of) reproducibility can undermine confidence in the scientific process from the general public and/or allow special interest groups to manipulate information to intentionally sow distrust.

`r msmbstyle::question_end()`


<!--chapter:end:09_ReprodCrisis.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: TRUE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Responsible Research is Reproducible Research

**Learning Objectives**

After completing this set of activities you should be able to

* Define three consecutive steps in data analysis/methods as 
  + Acquiring data
  + Processing data
  + Analyzing data
* Compare and contrast effects of using programs such as excel/other point & click programs compared to scripting languages for data analysis on reproducibility
* Recognize that employing tools for analysis differ in how reliable, accessible, and verifiable they are and how this limits how conducive they are for reproducible research.

You should have a `ReprodRes.Rmd` in your project directory. Use that file to work through this set of activties - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. For this module there will not be any coding involved (no code chunks!) instead you should work a little bit on your `Rmarkdown` skills.

In your document try using some of the formatting skills:

```{marginfigure}

You have various resources at your disposal, including the `Rmarkdown` [cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf) you received a handout for and [Chapter 27](https://r4ds.had.co.nz/r-markdown.html) in our Textbook. The easiest way to learn `Rmarkdown` is to try out the syntax and then `knit` your document to see if it turns out as you think it should!

```

* Use different level headers to structure your document
* include something in **bold** and something in *italics*.
* add numbered and unordered bullet points (maybe even try adding multiple levels if you're feeling adventurous).

For these activities we will do all the computing (thinking) with our own brains ... so let's dive straight in!

## What even are these 'Methods' you speak of?

`r msmbstyle::question_begin(label="ques:RR2-01")`

Think back on scientific papers you have read or lab reports you have written and list the main sections of the typical format you encounter.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-02")`

Give a brief description of what the "Methods" or "Method & Materials" section of a scientific paper or report contains and assess whether or not you think this section generally meets the standard you have just laid out.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-03")`

One framework to categorize components of the "methods" of a study is to place them into three steps that build on each other.

1. Acquire data
2. Process data
3. Analyze data

Use the example of our recent (re)analysis of the Shark Nursery study to describe what each of these steps entails.

`r msmbstyle::question_end()`


## Tools of the trade

Tiny history lesson Lotus123 is a  what launched IBM Personal Computers into offices around the world ... 

![Lotus123](figures/lotus123.png)

My very first PC adventures included running Lotus123 off of big floppy disks. Microsoft developed Excel and the Office package and quickly PCs where not only in offices and homes around the world. Spreadsheet applications were initially focused mainly on managing and organizing data (think HR department and payroll) but increasingly complicated calculations were possible and soon Excel snuck its way into scientific research.

Today, the tools used by scientists to analyze their data vary from highly specialized tools for very specific tasks (each with their own required data format), to large software packages like SAS and STATA Many of these tools have GUIs (graphic user interfaces) and are what is frequently referred to as "point & click" or "WYSIWYG"^[WYSISWG = What you see is what you get. Compare this to WYWIWYG = what you want is want you get.].

`r msmbstyle::question_begin(label="ques:RR2-04")`

Rely on your experience of having used programs like Excel, SAS/STATA, or other specialized programs in a statistics class or other lab courses you have taken to compare and contrast using those types of programs for data analysis to programing languages like R, Perl or Python. Discuss the benefits and drawbacks of both approaches.

`r msmbstyle::question_end()`


## Impacts of Flawed Data Analysis

### Case Study 1

`r msmbstyle::question_begin(label="ques:RR2-05")`

Pull up this [paper on the connection of CSF interleukin-6 and depression](https://www.sciencedirect.com/science/article/pii/S0889159113001414). Quickly skim the abstract and summarize the main result. Do this before untoggling the solution button for part 2 of this question.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Okay, fine - that was a little bit of a trick question; did you notice the retraction notice?

You can access a [more detailed version of the retraction here](https://www.sciencedirect.com/science/article/pii/S0889159113005424). Read the retraction notice and briefly describe why the paper needed to be retracted.

After the authors fixed their mistake they published a [corrected version of the paper](https://www.sciencedirect.com/science/article/pii/S0889159114001287). Compare the titles & abstracts of the retracted and updated papers to determine if their mistakes had a significant impact on the reported results.

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label="ques:RR2-06")`

Give a brief summary of key lessons you have learned from the case study.

`r msmbstyle::question_end()`


### Case Study 2

Before today's class you should have completed these readings:

* Shariff et al 2016 "[What is the association between religious affiliation and children's altruism?](https://www.sciencedirect.com/science/article/pii/S0960982216306704?via%3Dihub)"
* Retraction notices of Decety et al. 2015 "[RETRACTED: The Negative Association between Religiousness and Children's Altriusm across the World](https://www.sciencedirect.com/science/article/pii/S0960982215011677)"
* Optional: [How a study based on a typo made news everywhere - and the retraction didn't](https://www.vox.com/future-perfect/2019/10/3/20895240/study-typo-religion-children-generosity-retraction).


`r msmbstyle::question_begin(label="ques:RR2-07")`

Briefly discuss why the paper needed to be retracted; as part of your discussion compare and contrast the correspondence published by Shariff et al. to the retraction notice from the original authors Consider the following (and of course add anything else you noticed/think is important).

* Why was the paper retracted?
* What is the central statement being made in the correspondence/retraction notice?
* What is the tone of the correspondence/retraction notice?
* How could this mistake have been avoided?
* How could bias of both sets of authors/previous knowledge have played into the conclusions of the paper/noticing and identifying the mistakes (consider both sets of authors)?

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-08")`

The results of the initial study were widely published. By contrast, the retraction was not; matter of fact most people "doing their own research" on the interwebs are considerably more likely to come across an article describing the initial findings and we all know from personal experience how infrequently we look up the original study being described - so chances are they might never discover that the study has been retracted.

Use this specific example to illustrate what **Brandolini's Law** (i.e. the Bullshit Asymmetry Principle) is.

![Brandolini's Law](figures/BSasymmetry.png)

`r msmbstyle::question_end()`

`r msmbstyle::question_begin(label="ques:RR2-09")`

Give a brief summary of key lessons you have learned from this case study.

`r msmbstyle::question_end()`


### Case Study 3

Before today's class you should have completed key sections from [Herndon et al. 2014 "Does high public debt consistently stifle economic growth? A critique of Reinhart & Rogoff"](https://academic.oup.com/cje/article/38/2/257/1714018?login=true):

* Section 1 (Introduction)
* Section 2 (Public impact and policy relevance)
* Introduction to section 3. (Replication) + Section Headers
* Section 4 (Conclusion)

`r msmbstyle::question_begin(label="ques:RR2-10")`

Briefly summarize the major points of criticism of the original paper.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-11")`

Briefly discuss the tools that were used for analysis and determine how they could have contributed to the flawed analysis being published (remember, it did get through peer-review!)

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-12")`

Recall our initial classification scientific misconduct along a continuum of "Responsible - Ethical - Unethical/Misconduct" and use that to compare the flawed analysis laid out here to the issues that were uncovered in the previous two case studies. Consider factors including content, when/why mistakes were uncovered, and intent.

`r msmbstyle::question_end()`



`r msmbstyle::question_begin(label="ques:RR2-13")`

Consider the policy and decision-making implications of this (and the other two papers) and discuss if you think retracting the actual paper with mistakes in it is sufficient.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-14")`

Propose strategies that could be used to avoid flawed analysis demonstrated in this (and the other) case studies to be published in the first place and whose role it should be to "enforce" responsible conduct.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR2-15")`

Give a brief summary of key lessons you have learned from this case study.

`r msmbstyle::question_end()`


<!--chapter:end:10_ReprodRes.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: TRUE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Project Management Revisited

**Learning Objectives**

After completing this set of activities you should be able to

* Describe the benefits of a workflow that is organized, reproducible, and shareable
* Identify essential components of a reproducible research compendium
  + all raw data
  + metadata describing data/sources
  + code to extract, transform/load data, generate visualizations, and run analysis/fit models


You should have a `ResearchComp.Rmd` in your project directory. Use that file to work through this set of activties - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment. So, first thing in the `YAML` header, change the author to your name. You will use this `Rmarkdown` file to record your answers. For this module there will not be any coding involved (no code chunks!) instead you should work a little bit on your `Rmarkdown` skills.

In your document try using some of the formatting skills:

```{marginfigure}

You have various resources at your disposal, including the `Rmarkdown` [cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf) you received a handout for and [Chapter 27](https://r4ds.had.co.nz/r-markdown.html) in our Textbook. The easiest way to learn `Rmarkdown` is to try out the syntax and then `knit` your document to see if it turns out as you think it should!

```

* Use different level headers to structure your document
* include something in **bold** and something in *italics*.
* add numbered and unordered bullet points (maybe even try adding multiple levels if you're feeling adventurous).

For these activities we will do all the computing (thinking) with our own brains ... so let's dive straight in!

## An organized, reproducible and shareable workflow

In [Chapter 3](https://bi449fa21.netlify.app/project-management-and-rmarkdown-basics.html) of our labmanual you were introduced to Project management and Rmarkdown Basics. We said that the goal of open science and reproducible research is to make scientific methods, data, and results more transparent, available and reproducible. In that section, you were introduced to `Rprojects` and `Rmarkdown` and the structure of our Project Folders for this semester with the reasoning that this structure would allow you to establish a reproducible workflow that is organized, reproducible, and shareable.

`r msmbstyle::question_begin(label="ques:RR3-01")`

Briefly discuss the advantages of designing a workflow that fulfills the three-fold goal of being organized, reproducible, and shareable.

`r msmbstyle::question_end()`

## Research Compendium

`r msmbstyle::question_begin(label="ques:RR3-02")`

Briefly define what a research compendium is including central components/principles to be effective, then assess whether our project folder fulfills these criteria.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label="ques:RR3-03")`

Check out the [Research Compendium](https://github.com/dgs108/TX_EFH) for the Shark Nursery study that we reproduced in our last module and determine if it fulfills the criteria we have set for a "good" research compendium.

`r msmbstyle::question_end()`









<!--chapter:end:11_ResearchComp.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: TRUE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
  tidy = FALSE, 
  message = FALSE,
	warning = FALSE,
  cache.extra = packageVersion('msmbstyle'))

options(htmltools.dir.version = FALSE)

```

# Data Wrangling & Tidy data Revisited

**Learning Objectives**

After completing this set of activities you should be able to

* confidently distinguish between tidy and non-tidy data sets
* confidently use `pivot_longer()` and `pivot_wider()` to convert between tidy (long) and non-tidy (wide) data frames


Download the files I have dropped in the `#hwassignments` channel on slack and put them in your `data` folder (`.../02_ReproducibleResearch/data/`).

Then use `File > New File > R Script` or `Ctrl + Shift + N` to open an `R script` to document the code you are about to write as you practice some of your data wrangling and tidying skills ^[Remember, `R scripts` are "all code" compared to `Rmarkdown` files that are a combination of plan text and code chunks.]. 

Save your `R script` as `tidy.R` and at the top of your script start by loading the tidyverse packages.

```{r}

library(tidyverse)

```

As always - Comment your code!


## Round 1

The `tidyverse` set of R packages includes a few data sets we can use to play around with and sharpen our skills.

A typical messy/non-tidy data set is formatted for presentation as opposed to analysis. This means that we frequently find variables both in rows and in columns, and some of our columns headers are actually values not variable names.

Load a data set into your environment which contains information on income and religious affiliation for the US and take a look at the contents using `head()` and `View()`

```{r}

df <- relig_income

head(relig_income)

```

Remember, a tidy data set has three interrelated rules

1. Every variable has its own column.
2. Every observation has its own row
3. Every value has its own cell.

To tidy a data set you first want to take a look at your data and determine what the observations, variables, and values are. One of the most typical issues we find are that some of the column names are actually values, we might also have some columns that are redundant, or columns that contain more than one value.

`r msmbstyle::question_begin(label = "ques:t1")`

Use `View(df)` to take a closer the data set. Is it a tidy data set? Can you identify redundant columns? Are there columns that are likely values instead of variables? What variables can you identify that are already correctly formatted as columns?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

* variables: 
  + `religion` is definitely a variable
* all other column names appear to be values indicating income
* values in the cells apart from the religion column are frequency of observing a combination of religion and income.

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label = "ques:t2")`

What should our tidy data set look like conceptually (what variables should be included). What function can you use to make this a tidy data set?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

```{r}

tidy <- df %>%
  pivot_longer(names_to = "income", values_to = "frequency", 2:11)

head(tidy)

```

`r msmbstyle::solution_end()`


## Round 2

Load a new data frame - this one contains information on the data a song first entered the billboard top 100 and then tracks the rank for each successive week. 

This format is convenient for data entry - but is it tidy?

```{r}

df <- billboard

head(df)

```

`r msmbstyle::question_begin(label = "ques:t3")`

Use `View(df)` to take a closer the data set. Is it a tidy data set? Can you identify redundant columns? Are there columns that are likely values instead of variables? What variables can you identify that are already correctly formatted as columns?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

* variables: 
  + `artist`, `track`, and `date.entered` are definitely variables
* all other column names appear to be values indicating the week
* values in the cells of columns named `wkxx` are ranks.

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label = "ques:t4")`

What should our tidy data set look like conceptually (what variables should be included). What function can you use to make this a tidy data set?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Previously, we used column position to gather, we can also use column names. We can also use the argument `values_drop_na = TRUE` to drop missing values. In this example, not all songs stay in the billboard for all 76 weeks that are tracked, once we pivot to a tidy (long) format that will result in a lot of observations (rows) that are `NA`. By enabling the `values_drop_na` option it saves us a step of having to use `filter()` to get rid of those unnecessary rows.

```{r}

tidy <- df %>%
  pivot_longer(names_to = "week", values_to = "rank", wk1:wk76, values_drop_na = FALSE)

head(tidy)

```

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label = "ques:t5")`

How could you sort the data set by

* artist, track, week?
* date, rank?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Previously we used column position to gather, we can also use column names to indicate which columns are being gathered into a key-value pair. 

Helpful argument of `pivot_longer()` that we haven't explored yet is `values_drop_na = TRUE` to drop missing values. In this example, not all songs stay in the billboard for all 76 weeks that are tracked, once we pivot to a tidy (long) format that will result in a lot of observations (rows) that are `NA`. By enabling the `values_drop_na` option it saves us a step of having to use `filter()` to get rid of those unnecessary rows.

```{r}

tidy %>%
  arrange(artist, track, week) %>%
  head()

tidy %>%
  arrange(date.entered, rank)

```

`r msmbstyle::solution_end()`


## Round 3

You can load the `who` data set which contains tuberculosis (TB) cases by year, country, age, sex, and method diagnosed from the [2014 WHO Report](http://www.who.int/tb/country/data/download/en/).

Start by loading the data set into your environment and taking a quick look at the information contained.

```{r}

# load data set
df <- who

head(who)

```

Is this a tidy data set?

Nope - this is a typical, real-life messy data set... so, let's tidy it up.


`r msmbstyle::question_begin(label = "ques:t6")`

Use `View(df)` to take a closer the data set. Can you identify redundant columns? Are there columns that are likely values instead of variables? What variables can you identify that are already columns.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

* variables: 
  + `year` is definitely a variable
  + `country`, `iso2`, and `iso3` all specify country in different ways 
* all others appear to be values.

`r msmbstyle::solution_end()`

Our first step should be to gather all the columns that appear to be values^[whenever you're not entirely sure what the variable is, a good name for that column is `key`.], we know from the description of the data set that our values that are recorded in the data set are case numbers so let's call our column containing the values currently in those columns `cases`. You've also noticed that there are a lot of `NA` values so we will drop those when we pivot the table.

```{r}

tidy <- df %>%
  pivot_longer(names_to = "key", values_to = "cases", cols = 5:60, values_drop_na = TRUE)

head(tidy)

```

Note the difference in dimensions between `df` and `tidy`.

Okay, so what gives with those values in the format of `new_ep_f014` etc?

If you where to go to the documentation (meta-data) for this data set you would find that each of these codes for several pieces of information.

* first three letters indicates whether these are old or new TB cases (all of ours are new).
* the next two letters describe the type of TB (e.g. `rel` is relapse).
* sixth letter is the sex of TB patient.
* remaining numbers are the age group (e.g. 014 is 0 - 14)

`r msmbstyle::question_begin(label = "ques:t7")`

If the definition of a tidy data set is that each column is a single variable, what should our next step be? What function would you use to achieve this?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Correct, we must separate this column into multiple columns using `separate()` ^[There is an unhelpful formatting issue where instead of `new_rel` (easy to separate, and other entries have this format with the underscore) the entries are `newrel` so we will add a line of code using a function from the `stringr` package to fix that first.]

Because the values are not consistently separated using an `_`, we  have to use a two step process where we first create new column called `sexagegrp` and the separate that into `sex` and `agegrp` using positional information.

```{r}

tidy <- tidy %>%
  mutate(key = str_replace(key, "newrel", "new_rel")) %>%
  separate(key, into = c("new", "type", "sexage"), sep = "_") %>%
  separate(sexage, into = c("sex", "age"), sep = 1)
  
head(tidy)

```

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label = "ques:t8")`

Now we should consider whether there are unnecessary columns we can drop. Can you identify columns that are redundant or don't carry necessary information? What function can we use to drop them?

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

Exactly, `select()` can be used to remove columns, we don't need the column `new` since that is identical for all observations and we don't need three columns describing country information.

```{r}

tidy <- tidy %>%
  select(-new, -iso2, -iso3)

head(tidy)

```

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label = "ques:t9")`

Create a new data frame that contains TB cases for the United States the year 2000 for women. 

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}

tidy %>%
  filter(country == "United States of America" & year >= 1990 & year <= 2000 & sex == "f") %>%
  head()

```

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(label = "ques:t10")`

Create a table for easier comparison of cases across age groups by year. What would that table need to look like? What function(s) will you use.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

We pivot!

```{r}

tidy %>%
  filter(country == "United States of America" & year >= 1990 & year <= 2000 & sex == "f") %>%
  pivot_wider(names_from = "age", values_from = cases)

```

`r msmbstyle::solution_end()`

<!--chapter:end:12_TidyData.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
---

```{r include=FALSE}

library(msmbstyle)

# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('msmbstyle'),
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE)

options(htmltools.dir.version = FALSE)

source("scr/ggplot.R")

```

# Climate Change

**Learning Objectives**

After completing this tutorial you should be able to

* analyze earth's global temperatures to determine if they are increasing
* analyze CO<sub>2</sub> data to determine if atmospheric levels are increasing
* compare rates of increase of CO<sub>2</sub> and global temperature
* compare current trends of with rates of change during pre-historic periods using ice core data
* interpret results to understand current climate change

Download the [03_ClimateChange](https://drive.google.com/drive/folders/100Zbhu8l_euEA9lyKWXtcfMhhQpn8vWb?usp=sharing) project folder. Once you have downloaded it, unzip the project directory into your `BI449` directory. You can open the `Rproj` for this module either by double clicking on it which will launch `Rstudio` or by opening `Rstudio` and then using `File > Open Project` or by clicking on the `Rproject` icon in the top right of your program window and selecting `Open Project`. 

Once you have opened a project you should see the project name in the top right corner^[Pro tip: If you run into issues where an Rmarkdown won't render or file paths aren't working (especially if things were working previously) one of your first steps should be to double check that the correct `Rproj` is loaded.].

You should have a `ClimChange.Rmd` in your project directory. Use that file to work through this set of activities - you will hand in your rendered ("knitted") rmarkdown file as your homework assignment.  

You will use this `Rmarkdown` file to record your answers. So, per usual, first thing in the `YAML` header, change the author to your name. Remember to use comments to describe your code; at minimum you should have one comment per code set^[You should do this even if you are adding code yourself or if you are using code from our manual, even if it isn't commented in the manual ... you may of course add as many comments as you need to be able to recall what you did]. Now that you have gained additional `Rmarkdown` skills use those to format your "report" that you will hand in. Code chunks should have comments, add answers to questions in plain text (not as code in chunks), use headers intentionally, and of course you have options like bullet points etc. as well.

Global change biology/ecology is an emerging field of research that is focused how changes in temperature and precipitation resulting from anthropogenic climate change will in turn alter the distribution, abundance, behavior, and physiology of on individual species, populations, communities, and ecosystems. In recent years, modeling has become increasingly sophisticated. Additionally, long-term and large-scale data sets are becoming increasingly available. This allows us to not only investigate impacts that can be observed now but also predict how different scenarios of global change will impact the biosphere. Typical lines of investigation include range shifts and invasions, how communities can be altered and the formation of new ones, impacts of urbanization, wildfires, and other natural catastrophes, loss of biodiversity, and changes in phenology along with understanding mechanisms of species/ecosystem resilience, and the interplay of biosphere, atmosphere, and hydrosphere. Many problems have both a temporal and spatial aspect making them especially complex and difficult to model. 

During this unit we will start by taking a look at past and present rates of change of global temperature and CO<sub>2</sub> concentrations. Then we will take a look at some additional data sets to sharpen our visualization skills. 

Let's start by loading the packages we will need for this tutorial.

```{r}

# load libraries
library(knitr)             # kable() to make pretty tables
library(tidyverse)         # includes readr, tidyr, and dplyr to wrangle data & ggplot to plot
library(plotly)            # makes interactive plots possible
library(patchwork)

# turn of sci notation
options(scipen=999)

```


## Investigating Climate Change

Climate regimes are defined by patterns of temperature and precipitation. Global temperature is a function of how much energy the earth receives and stores which in turn is influenced by three major factors:

1. The amount of energy received from the sun.
2. Reflection of energy by earth's surface.
3. Atmospheric composition (greenhouse gas effect).

The most recent [IPCC Asessement](https://www.ipcc.ch/assessment-report/ar6/) has made two very clear statements regarding climate change:

1. Climate change “unequivocally” caused by humans.
2. The impact of anthropogenic climate change is “unprecedented”.

Based on their assessment of studies having looked at global warming and climate change to date, the global mean warming is estimated at 1.1°C compared to the pre-industrial record. Additionally, under every emission scenario and average warming of 1.5 °C is likely to be reached in in next 20 years. Finally, some effects such as melting glaciers, ice sheets and the permafrost have been deemed irreversible.

Current climate change is concerning not (only) because of absolute warmth of the earth but due to rate at which it is occurring. What this means is that over the history of our planet there have been times at which temperatures have been higher but what is "unprecedented" is the **rate** of increase and impact that is having on our planet^[We will look at examples of that impact using various case studies.]

The reason why the IPCC describes current climate change as **anthropogenic** is that increasing levels of atmospheric CO2 are driving the rapid increase in temperatures. Earth absorbs incoming solar radiation at its surface and emits long-wave radiation to maintain the energy balance at the surface. Only as small portion of that emitted radiation goes directly into space, most of it is absorbed by greenhouse gases (e.g. CO2) in the atmosphere. For the atmosphere to maintain its energy balance it emits radiation to space and back to earth. With increasing concentrations of GHG, the atmosphere absorbs and re-emits increasingly more energy. This creates an imbalance at earth's surface and as a response earth continues to emit more energy to re-balance the budget and as a result global temperatures increase.

<br>

![The atmospheric energy budget source weather.gov](figures/energy_balance.jpg)

<br>

Before we get started let's consider what data sets we need to investigate whether rates of temperature increase are indeed "unprecedented", to identify patterns of atmospheric CO2 concentrations consistent with anthropogenically driven climate change, and what we would expect our results to look like if the IPCC assessment is indeed correct in their claims.

* what variables/measurements do we need?
* what comparisons do we need to be able to make to determine if the rate of temperature increase is unprecedented?
* what would patterns consistent with anthropogenically driven climate change look like?

We also need to consider limitations of our approach. In this case, we will need to consider whether our approach is investigating a causal or mechanistic effect in and of itself or if we are uncovering evidence consistent with a known mechanism/process.


## Current rates of air temperature change

Let's start by taking a look at changes in global mean air temperatures.

`r msmbstyle::question_begin(label = "ques:cch-1")`

Sketch out what the patterns of air temperature over time would look like if the (A) earth is warming, (B) cooling, (C) not changing at all and discuss with your class mates. Determine what variables you would plot on the x-axis, y-axis and what the slope would look like.

You do not need to include the sketches but practice describing figures with a 1-2 sentence description of what each scenario would look like.

`r msmbstyle::question_end()`

The air temperature data we will be using is compiled by the Goddard Institute for Space Studies (NASA) and can be accessed in [on their webpage](https://data.giss.nasa.gov/gistemp/.) which also describes how their data set was compiled and processed. Scroll down the page 
to the section `Tables of Global and Hemispheric Monthly Means and Zonal Annual Means` and download the [`CSV`](https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv) version of the `Global-mean monthly seasonal, and annual means`.


`r msmbstyle::question(label = "ques:cch-2", text = "This data is presented as temperature anomalies, i.e. as deviations from the corresponding 1951 - 1980 mean. Explain what this means and argue why this is a good way to present this data.")`

After you have downloaded the data set place it in your `data` folder in your project directory and read it into your `Global Environment`.


`r msmbstyle::question(label = "ques:cch-3", text = "This is a csv formatted file, how can we read it into R?")`


`r msmbstyle::solution_begin()`

Correct! You need to use the `delim` argument. Let's try to read in the file.

```{r}

# read csv file
temperature <- read_delim("data/GLB.Ts+dSST.csv", delim = ",")

```

`r msmbstyle::solution_end()`

Before we move on, let's check our `temperature` object in the `Global Environment` to see if the file read in okay - spoiler alert, it did not^[You should create a habit of always checking that your data has read in as expected, immediately determining that something is wrong and correcting it will minimize issues with troubleshooting down the line.].

**Troubleshooting Skills**: Your file didn't read in correctly. Let's figure out why. 

`r msmbstyle::question(label = "ques:cch-4", text = "What ideas do you have for us to track down the issue? Document the process you used to identify and fix the issue using short bullet points for future reference.")`


`r msmbstyle::solution_begin()`

A good starting point is to open the file in a text editor to see if we missed anything. Sure enough there seems to be an additional line at the beginning which is probably causing the issue. One way to fix this is to simply delete the extra line ... but if we want to have a reproducible workflow we want to avoid manually editing our data sets. 

Instead use `?read_delim` to pull up the help file for the function. You should find an argument called `skip` which will let us tell the function how many extra lines there are at the beginning of the file.

```{r}

# read csv file
temperature <- read_delim("data/GLB.Ts+dSST.csv", delim = ",", skip = 1)

head(temperature)

```

That seems to have done the trick.

`r msmbstyle::solution_end()`

Let's take a slightly more detailed look at our data set to make sure there aren't any additional changes we need to make, for example we need to determine if there are `NA` values and if they are properly formatted and we need to make sure all the columns are `numeric`.

`r msmbstyle::question(label = "ques:cch-5", text = "Take a look at your data frame, make sure that you know what information each column contains, identify if anything is out of order, and figure out how to clean up any issues.")`

`r msmbstyle::solution_begin()`

`NA` values have been specified as `***`, which has caused some columns to be formatted as `character` instead of `numeric`. We can use `replace()` to search every column (we specify this using `.` instead of a specific column name) and `mutate_if()` which tells R to check every column and if it is a character data type (`is.character`) to convert it to numeric (`as.numeric`).

```{r}

temperature <- temperature %>%
  replace(. == "***", NA) %>%
  mutate_if(is.character, as.numeric)

```

`r msmbstyle::solution_end()`


`r msmbstyle::question(label = "ques:cch-6", text = "Now that we are all set, let's create a plot with the yearly mean global temperature anomaly across time. Conceptually, what parameters do we need to plot on the x and y-axis to accomplish this?")`

`r msmbstyle::solution_begin()`

We are going to use `ggplot2` which is part of the `tidyverse` to plot our figure. This will introduce you to some of the standard syntax ^[Fear not, we will work through the details of the framework on which `ggplot2` relies later on].

```{r eval=FALSE}

ggplot(temperature, aes(x = Year, y = `J-D`)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(color = "red", size = 1.5, se = FALSE) +
  labs(x = "year", y = "temperature anomaly [C]",
       title = "Annual mean global surface temperature relative to 1951-1980 average."
       caption = "source: NASA Goddard Institute for Space Studies")

```


```{r caption="Change in annual mean global surface temperature from 1880 - 2021 relative to 1951-1980 average.", echo=FALSE}

ggplot(temperature, aes(x = Year, y = `J-D`)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(color = "red", size = 1.5, se = FALSE) +
  labs(x = "year", y = "temperature anomaly [C]",
       title = "Annual mean global surface temperature relative to 1951-1980 average.",
       caption = "source: NASA Goddard Institute for Space Studies") +
  theme_standard

```

`r msmbstyle::solution_end()`

`r msmbstyle::question(label = "ques:cch-7", text = "Describe & interpret the figure: Describe the change in the deviation of global temperature to the 1950-1980 mean. Include and explanation of what it means for values to be negative or positive.")`

This figure gives us a qualitative view of changing global temperatures. To quantitatively assess the rate of change, we can fit a linear regression as `y = mx + b`. With `m` as the slope and `b` as the intercept to determine the rate of change. 

For this example you would define the rate of change as the change in temperature divided by the change in time for a certain time period. A more general definition would be that you are calculating the **slope** of the line you have fit through the data as the change-in-y divided by the change-in-x^[If you compare the two figures you should see that fitting a linear regression is an oversimplification but it will allow us to make a quantitative comparison].

```{r eval=FALSE}

ggplot(temperature, aes(x = Year, y = as.numeric(`J-D`))) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", color = "red", size = 1, se = FALSE) +
  labs(x = "year", y = "temperature anomaly [C]",
       title = "Annual mean global surface temperature relative to 1951-1980 average.",
       caption = "source: NASA Goddard Institute for Space Studies")

```

```{r fig.cap="Linear regression (red) for change in annual mean global surface anomaly from 1880-2021 (blue). Temperature anomaly measured relative to 1951-1980.", echo=FALSE}

ggplot(temperature, aes(x = Year, y = as.numeric(`J-D`))) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", color = "red", size = 1, se = FALSE) +
  labs(x = "year", y = "temperature anomaly [C]",
       title = "Annual mean global surface temperature relative to 1951-1980 average.",
       caption = "source: NASA Goddard Institute for Space Studies") +
  theme_standard

```

We can fit the linear regression using the function `lm()`.

```{r}

# fit linear regression
score_model <- lm(`J-D` ~ Year, data = temperature)

# view summary of results
summary(score_model)

```

`r msmbstyle::question_begin(label = "ques:cch-8")`

We are interested in the equation of the regression line, how well the data fits the line, and whether or not the regression is significant. We can use the output of the linear regression to determine that. The estimate columns shows the values for the intercept `b` and the slope `m` for the variable (in this case year).

Determine what the equation for our linear regression looks like and use this to determine the rate of change, including what the units would be.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:cch-9")`

The adjusted R2 value describes the proportion of variance of the dependent value explained by the independent variable. In our figure what is the dependent and the independent value? How well does the data fit our regression? 

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:cch-10")`

The column `PR(>|t|)` is the probability of the coefficient for the variable. Is our regression significant?


`r msmbstyle::question_end()`


`r msmbstyle::question(label = "ques:cch-11", text = "Interpret the results of your analysis to determine whether you have evidence of a warming earth.")`


`r msmbstyle::question_begin(label = "ques:cch-12")`

Now you! 

1. Identify the time period where you observe the steepest rate of increase.
2. Create a subset of the data set to contain only that time frame. 
3. Plot you subset of data with a linear regression. 
4. Perform a statistical analysis to determine the rate of change.
5. Summarize your results and compare them to your results above.

`r msmbstyle::question_end()`


## Rate of change of atmospheric CO<sub>2</sub>

Next, let's determine the rate of change of atmospheric CO<sub>2</sub>. 

Dr. Charles David Keeling (1928 - 2005) began collecting atmospheric CO<sub>2</sub> concentration data at the Mauna Observatory (Hawaii); this data set comprises the longest measurement of atmospheric CO<sub>2</sub> concentrations^[Longest measurement using instruments - we will see later that we can use proxy data to indirectly measure CO<sub>2</sub> levels for much longer time periods]. This data set has been fundamental to our understand the role of human activities such as fossil fuel burning in driving climate change.

`r msmbstyle::question(label = "ques:cch-13", text = "Briefly argue why Hawaii is a good location for this.")`

We can access the data set directly from the [Global Monitoring Laboratory](https://gml.noaa.gov/ccgg/trends/). Select the `Data` tab, then download the `csv` data set `Mauna Loa CO2 annual mean data`.

Once you have downloaded the data set, make sure to move it to the `data` folder of your project directory, then read it into `Rstudio`.

```{r}

CO2 <- read_delim("data/co2_annmean_mlo.csv", delim = ",", skip = 55)

head(CO2)

```

`r msmbstyle::question(label = "ques:cch-14", text = "Explore your data set to determine if it is correctly formatted and briefly describe what information it contains - bullet points are fine.")`

`r msmbstyle::question(label = "ques:cch-15", text = "Plot the change in CO2 over time and use a linear regression to determine the rate of change in atmospheric CO2 over the entire data set based on your analysis. Argue how confident you are in these results.")`

`r msmbstyle::question(label = "ques:cch-16", text = "Compare your results of rates of change of average global temperature and atmospheric CO2 and describe the phenomenon that can be used to explain this.")`

```{r fig.cap="Comparison of increase in mean global temperature anomaly relative to 1951-1980 (left) and increase in mean atmospheric CO2 concenttrations (right panel) measured a Mauna Loa Observatory. Regression lines indicating temporal trend included in red.", fig.height=4, echo=FALSE}

p1 <- temperature %>%
  filter(Year >= 1959) %>%
  ggplot(aes(x = Year, y = `J-D`)) +
  geom_line(color = "blue", size = 1.5) +
  geom_smooth(color = "red", size = 1, se = FALSE) +
  labs(x = "year", y = "temperature anomaly [C]",
       caption = "source: NASA Goddard Institute for Space Studies") +
  theme_standard

p2 <- ggplot(CO2, aes(x = year, y = mean)) +
  geom_line(color = "blue", size = 1.5) +
  geom_smooth(color = "red", size = 1, se = FALSE) +
  labs(x = "year", y = "mean atmospheric CO2 [ppm]",
       caption = "source: Global Monitoring Laboratry/NOAA") +
  theme_standard

p1 + p2

```


## Comparison of current and pre-historic rates of change 

The most recent IPCC assessment has labeled the increase in temperatures driving contemporary climate change as "unprecedented". However, temperatures have changed in earth's past, and temperatures have even been higher than what we are experiencing now. What is unprecedented is the rate at which this is occurring. 

Shall we investigate?

To do this we will need to look at past climate change. The two data sets we just took a look at are measurements of temperature and CO<sub>2</sub> using instrumentation, i.e. we have directly measured values for the parameters we are interested in at different points in time. Dr. Keeling was one of the first scientists to consider the importance of long-term monitoring sites. The rapid changes taking place in our environment have created a push to generate long-term data sets with a focus on making the accessible. We will take a look at some of these data sets later in this semester and you will likely use at least one of them for your own data science project.

However, we frequently have questions that might extend beyond data sets like the two we used above.

How can we access data from before we had instrumentation? One way to do this we have to use so-called [proxy data sets](https://www.ncdc.noaa.gov/news/what-are-proxy-data).

`r msmbstyle::question(label = "ques:cch-17", text = "In general, a proxy is an intermidary or subsistute, i.e. it is a parameter that can be used to represent the value of something in a calculation. Paleoclimatologist use preserved physical characteristics of the environment to stand in for direct measurements using instruments, typical examples would be ice cores, tree rings, ocean sediments, or fossil pollen. Briefly discuss the pros and cons of proxy data compared to direct measurements using specialized intruments.")`

For a deep dive on proxy data sets and climate archives, you can check out [a detailed interactive brief on proxy data](https://interactive.carbonbrief.org/how-proxy-data-reveals-climate-of-earths-distant-past/).

For our current assessment we are going to explore proxy data derived from ice cores which next to ocean sediments give us some of the longest records of past climate conditions. 

Hundreds of ice cores extracted from polar ice have proven valuable to understanding changes in atmospheric chemistry over pre-historic time. Here, we can make use of the fact that as the ice is formed, air bubbles are trapped. Because these air bubbles have remained frozen, they still have the same composition of gases as at the time they were trapped. The depth of an ice core is correlated to time, deeper ice is older. In other words, ice cores form an archive of atmospheric conditions over time. We can directly measure CO<sub>2</sub> from the air bubbles trapped in the ice and we can measure isotopic ratios of oxygen in the water molecules of the core to derive temperature.

Vostok Ice core data set has been constructed using ice cores from the Vostock research station in the Antarctica and can be access through the [Carbon Dioxide Information Analysis Center](https://data.ess-dive.lbl.gov/portals/CDIAC).

Let's start by taking a look at the temperature data. Use the code below to download the data and read it into R as a data frame^[We are using a slightly different method from before which allows us to directly download the data into our data folder. We are also using `read_table2()` from the `readr` package due to the fact that our text file is formatted using neither white space nor tabs.].

```{r}

# define url
url <- "http://cdiac.ess-dive.lbl.gov/ftp/trends/temp/vostok/vostok.1999.temp.dat"

# download file
download.file(url, "data/vostok_temp.txt")

# load dataset
vostok_temp <- read_table2("data/vostok_temp.txt",
                     skip = 60,
                     col_names = c("depth", "age_ice", "deuterium", "temperature"))

```

`r msmbstyle::question(label = "ques:cch-18", text = "Take a look at the data set and briefly describe what data is contained in the data set (you may want to take a peak at the original text file to get a better understanding).")`

Now we can create a plot of the temperature data over time. The age is recorded as years before present. For better visualization we will convert this to "thousand years ago" by dividing that number by 1,000.

```{r fig.cap="", fig.height=6, eval=FALSE}

ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = 1) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       title = "Temperature variation during glacial/interglacial periods",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)")

```


```{r fig.cap="Temperature variation during glacial/interglacial periods derived from air bubbles in Vostock ice cores.", fig.height=6, echo=FALSE}

ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = 1) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       title = "Temperature variation during glacial/interglacial periods",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)") +
  theme_standard

```

`r msmbstyle::question_begin(label = "ques:cch-19")`

Before we move at looking at rates of change, we first need to determine how to properly read this plot. Consider the following:

* What does it mean that time (on the x-axis) is represented as "thousand years ago" or "time before present"? How does this differ from the other time series we have plotted today?
* Temperature is being measured by proxy by looking at differences in isotope ratios; the data file lists this information as "Temperature variation". What does 0C mean on this plot?
* Consider how long glacial and interglacial periods typically last - are we currently in a glacial or interglacial period?
* In what parts of the figure is temperature increasing/decreasing (consider slope)?

`r msmbstyle::question_end()`

`r msmbstyle::question_begin(label = "ques:cch-20")`

Now add a trend line to our ice core temperature data and run a linear regression model. Argue whether or not you think this trend line is a good representation of long-term temperature change.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Plot temperature data derived from vostok ice cores.

```{r eval=FALSE}

ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       title = "Temperature derived from Vostok ice core, Antarctica.",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)")

```

```{r caption = "Change in temperature during glacial/interglacial periods derived from Vostok ice cores (blue) with fitted linear regression (red).", fig.height=6, fig.width=10, echo=FALSE}

ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       title = "Temperature derived from Vostok ice core, Antarctica.",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)") +
  theme_standard

```

And fit linear regression.

```{r}

# fit linear regression
score_model <- lm(temperature ~ age_ice, data = vostok_temp)

# view summary of results
summary(score_model)

```

*Remember, when you "argue" an answer you need to state your conclusion and then support that statement.*

`r msmbstyle::solution_end()`

In order to determine if the currently observed rate of change is "unprecedented" or not, let's identify past time periods with the fastest rate of change and calculate them.

To do this I will show you a little trick to plot an interactive figure.

```{r fig.height=6}

p1 <- ggplot(vostok_temp, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = .75) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)")

ggplotly(p1)

```

Now you can use your cursor to identify individual points on the plot to select the subset you want plot.

```{r eval=FALSE}

# define time range
min_year_vostok <- 128357
max_year_vostok <- 138193

# filter data set + plot
vostok_temp_subset <- vostok_temp %>%
  filter(age_ice >= min_year_vostok & age_ice <= max_year_vostok)

ggplot(vostok_temp_subset, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       title = "Temperature change recorded in Vostok ice core (128357 - 138193 years ago).",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)")

```

```{r fig.cap="Temperature trends (linear regression, red) recorded in Vostok ice core for time period from approx. 138 - 128 kyrs before present.", echo=FALSE}

# define time range
min_year_vostok <- 128357
max_year_vostok <- 138193

# filter data set + plot
vostok_temp_subset <- vostok_temp %>%
  filter(age_ice >= min_year_vostok & age_ice <= max_year_vostok)

ggplot(vostok_temp_subset, aes(x = age_ice/1000, y = temperature)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "thousand years before present", y = "Temperature variation [C]",
       title = "Temperature change recorded in Vostok ice core.",
       caption = "Data: Carbon Dioxide Information Analysis Center (CDIAC)") +
  theme_standard

```

Next, fit a linear model to your subset

```{r}

# fit linear regression
score_model <- lm(temperature ~ age_ice, data = vostok_temp_subset)

# view summary of results
summary(score_model)

```

`r msmbstyle::question(label = "ques:cch-21", text = "Identify two more ranges with increasing temperatures and determine the rate of change during that time period. Pro tip: Identify another time range - then ask the person next to you what time period they ran.")`

Now, let's take a look at past patterns of changes in atmospheric CO<sub>2</sub> over time^[I dropped the vostok_ice.txt file in our slack channel, you will need to download it and move it into your data folder. Then you can use the code using read_delim() below to read it in.].

```{r eval=FALSE}

# define url
url <- "http://cdiac.ess-dive.lbl.gov/ftp/trends/CO2/vostok.icecore.CO2"

# download file
download.file(url, "data/vostok_ice.txt")

# load dataset
vostok_ice <- read_delim("data/vostok_ice.txt", delim = "\t",
                     skip = 21,
                     col_names = c("depth", "age_ice", "age_air", "CO2"))

```

`r msmbstyle::question(label = "ques:cch-22", text = "Select one time period with rapid increase in CO2 concentrations and apply what you have learned identifying past periods of rapid temperature increase to calculating the rate of change.")`

## Final Conclusions

Now it's time to put everything together. You might want to refer back to the beginning of this chapter when we looked at some background information about the IPCC report, the atmospheric energy budget, formulated our central questions, thought about what data sets when can use to answer our question, and the limitations of our approach.

`r msmbstyle::question_begin(label = "ques:cch-23")`

Before we summarize and then interpret our results, let's re-orient ourselves to what we've done with this analysis.

* Write out the central question(s) we are asking.
* List the data sets you used to investigate and what metric you calculated for each.

`r msmbstyle::question_end()`

Well-written papers frequently end their introduction section with a paragraph that summarizes what their study is investigating and how they are investigating that (set of) questions - it forms a "bridge" between the introduction that sets up relevant background information (why is my question important?) and the methods section which is a very detailed account of how data was acquired (experimental design), processed, and analyzed.

If you read several of these paragraphs you will realize that they all contain a statement that follows a general formula along these lines:

> In this study, we investigated [CENTRAL QUESTION OR HYPOTHESIS]. To do this we used [DESCRIPTION OF THE TYPE OF DATA SET GENERATED] to [METRICS THAT WERE CALCULATED].

You should always be able to make a 2-3 sentence statement summarizing what you are investigating and how you did it, it's a good self-check that you know what you're trying to accomplish.


`r msmbstyle::question(label = "ques:cch-24", text = "Use your results to compare current changes over the last approx. 200 years in atmospheric and CO2 concentrations and global temperature to pre-historic changes.")`

*Be strategic in how you structure your answer to put together all the parts of our analysis. For example, start with temperature and compare the rate of change you calculate for recent times to the rate of change you calculated during the more distant past (glacials/interglacials)^[ Remember to include units!]. In include your own rate of change + the answers your classmates posted in the slack channel. Then make a statement about whether or not we are currently observing an "unprecedent" change. Then do the same for CO2 concentrations. At this point you are just summarizing and describing your results, you can make clear statements of whether or not rates are positive/negative (your variables are increasing/decreasing), and whether you see the same/opposite trends but you should not yet interpret what those results mean. This would be equivalent to the results section of a lab report or research paper.*

`r msmbstyle::question(label = "ques:cch-25", text = "Interpret your results to assess whether recent changes in temperature are due to natural vs anthropogenic factors. For your answer consider both arguments that attribute the change to anthropogenic factors and natural fluctuations.")`

*Now you are entering the interpretation/discussion component of your analysis. At this point your now what your key results are and you need to figure out what they mean; essentially you are finally getting around to answering those key questions you asked before designing your experiment/study. You are going to use your results as evidence for/support of your conclusion. This means that you need to demonstrate that your results are consistent with your conclusion. You also want to consider alternative explanations, limitations of your data etc. Of course, ideally you can exclude those based on e.g. evidence from other studies/data sets.*

</br>


</br>


</br>


## Acknowledgments

These activities are based on the EDDIE Climate Change Module^[O'Reilly, C.M., D.C. Richardson, and R.D. Gougis. 15 March 2017. Project EDDIE: Climate Change. Project EDDIE Module 8, Version 1.]








<!--chapter:end:13_ClimateChange.Rmd-->

---
title: "Data Science for the Natural Environment (FA21)"
subtitle: "BI449 Data Science for the Natural Environment"
author: "Shannon J. O'Leary"
date: "`r Sys.Date()`"
knit: "bookdown::preview_chapter"
output:
  msmbstyle::msmb_html_book:
    highlight: tango
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    margin_references: FALSE
    css: msmb.css
bibliography: labmanual.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}

library(msmbstyle)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('msmbstyle'),
                      warning = FALSE,
                      message = FALSE)


options(htmltools.dir.version = FALSE)

source("scr/ggplot.R")

```

# Visualization


**Learning Objectives**

After completing this tutorial you should be able to

* apply the grammar of graphics implemented in `ggplot` to create plots.
* use core geometries implemented `ggplot` to produce scatter plots (`geom_point()`), line plots (`geom_line()`), bar plots (`geom_bar()`), histograms (`geom_histogram`), and box plots (`geom_boxplot()`).
* customize aesthetics of a plot (including color and axis labels).
* describe what faceting is and apply this to a plot.
* combine multiple plots in a multi-panel plot using `patchwork` and save plots to file.

You should have a `IntroVisualization.Rmd` in your project directory. Use that file to work through this set of activities - you will hand in your rendered ("knitted") Rmarkdown file as your homework assignment. In the `YAML` header, change the author to your name and make your formatting decisions such as theme, including a table of contents etc. Apply your `Rmarkdown` skills to format your "report" that you will hand in. Code chunks should have comments, add answers to questions in plain text (not as code in chunks), use headers intentionally, and of course you have options like bullet points etc. as well.

The majority of this set of activities is exploring how to use `ggplot()` to generate different types of plots. Each plot involves at least two lines of code - as we go through each step record the code presented and include a comment specifying what each line of code does. Comments can be inserted either between lines or at the end of each line.

Let's start by loading the packages we will need for this set of activities.

Before you can load all the libraries we need, you will need to install the `patchwork` package using `install.package("patchwork")`. You only need to do this once, the easiest wat to do this is to simply run that function directly in the console.

```{r}

# load libraries
library(knitr)
library(tidyverse)
library(patchwork)

# turn of sci notation
options(scipen=999)

```


## A grammar of graphics

Now that we are confident(ish) in our data wrangling the next step is learning how we can visualize our data for exploration and for communication. 

Exploratory analysis is an important component of data science as it not only gives you an overview of what information is contained in a data set, but it can also help you refine the question you are asking. Finally, once you have refined and completed an analysis, visualizations are a key component to communicate your results.

`ggplot2` is the core package of the `tidyverse` used for visualization. Similar to `tidyr` and `dplyr` having been built on the concept of a tidy data set, `ggplot2` was built on a framework which follows a layered approach to describe and/or build any type of visualization in a structure way termed the "grammar of graphics".

The grammar of graphics breaks down the components of any visualization into seven components.

* **Data** - the component of data set to visualize
* **Aesthetics** including axes, plot positions, and encodings such as size, shape, color
* **Scale** e.g normalizing values
* **Geometric objects** including points, lines, bars, ... to represent data
* **Statistics**, including confidence intervals, means, quantiles, ...
* **Facets**, i.e. subplots based on multiple dimensions
* **Coordinate system**, e.g cartesian, polar, ...

Until now you've probably thought of each plot type as it's own distinctive format (a scatter plot, a pie chart, etc) and have not considered that each plot can be broken down into these fundamental components, so at first this way of thinking of plots will likely not seem intuitive. 

But if you commit to thinking about plots in this abstract way you will quickly learn how these components fit together and realize the flexibility that thinking about plots in this way will give you to quickly generate exploratory plots, optimize your visualizations and be able to generate pretty much any plot you can think up^[You will also see that keeping your data `tidy` will allow you to customize your plots using different aesthetics and mappings to group your data.].


## Building plots

Y'all ready for this? 

Then, let's go back to our global mean temperature anomaly data set to figure out how the grammar of graphics can be applied and plot some data!

```{r}

temperature <- read_delim("data/GLB.Ts+dSST.csv", delim = ",", skip = 1) %>%
  replace(. == "***", NA) %>%
  mutate_if(is.character, as.numeric)

```

Recall how we created a line plot:

```{r}

ggplot(temperature, aes(x = Year, y = `J-D`)) +
  geom_line()

```

We can generalize this into a basic template that can be used for different kinds of plots, including scatter plots, bar plots, and box plots.

```

ggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) +
  <GEOM_FUNCTION>

```

Essentiall, the minimum components needed to make a plot are 

1. A data set (`<DATA>`), this will be a `data.frame` or a `tibble`^[Remember, when we use `read_delim()` our data is automatically a object of the format `tibble` and `data.frame`; a `tibble` has some additional properties but we can use them interchangeably].
2. Based on the data, we need to define what variables we are going to plot (`<MAPPINGS>`) and how we want them to be represented, i.e. what individual data points should look like and how they are encoded.
3. We need to define the geometries, i.e. what type of plot (`<GEOM_FUNCTION`). 

Let's break down what is happening to get a better understanding of how these minimum components fit together to create individual plots.

First, we use the `ggplot()` function to define the specific `data.frame` to use to build the plot using the `data` argument (`<DATA>`)^[Remember, that when we are using arguments in the specified sequence that they are defined you do not explicitly need to call them.].

In our example that would be

```{r fig.cap="Empty canvas generated using ggplot() function."}

ggplot(data = temperature)

```

Not much happens when you execute that piece of code other than creating a blank canvas in your plot panel^[If you are using and `Rmarkdown` document and have `Chunk Output Inline` enabled it will print beneath your code chunk]. 

That is because we still need to define an aesthetic mapping using the `aes()` function (`<MAPPING>`). 

Here, we are selecting the variables that we want to plot (columns in our `data.frame`). At minimum we need to specify what columns we want to plot on the x and y axis, but we can also define variables (columns) we want to use to encode using color, shapes, size etc.

```{r fig.cap="Coordinate system plotted after specifying mapping aesthetics, specifically which variables to plot on each axis."}

ggplot(data = temperature, aes(x = Year, y = `J-D`))

```

When we execute that code you will now get an empty coordinate system... This still seems like slim pickings but patience, young grasshopper, we have not yet defined the `geom`, i.e. what type of plot ("geometries", `<GEOM_FUNCTION>`) we want to use. For example, here we want generate a line plot (`geom_line()`).

For `ggplot` we add layers using the `+` operator, which is a pipe (similar to `%>%`) which tells `R` "and now add this".

```{r fig.cap="Line plot (geom_line()) showing global mean temperature anomalies relative to 1951-1980 average."}

ggplot(data = temperature, aes(x = Year, y = `J-D`)) +
  geom_line()

```

Now, we're playing!

One of the advantages of the layered framework of `ggplot2` is that we can plot multiple layers in the same plot by adding additional `geoms`. 

For example, we can plot a scatterplot and line plot in the same plot as such:

```{r fig.cap="Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point()."}

ggplot(data = temperature, aes(x = Year, y = `J-D`)) +
  geom_line() +
  geom_point()

```

Now that you know how to generate a simple plot, let's think about how we can further modify it to improve your visualization.

For example we can change the color, fill, size, and shape for each `geom` layer^[In this example we will use the arguments for individual geom functions to change how data points are represented using colors and shapes for each layer, we can also use `aes()` to set mapping aesthetics for the entire plots.].

```{r fig.cap="Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified."}

ggplot(data = temperature, aes(x = Year, y = `J-D`)) +
  geom_line(color = "darkblue", size = 1) +
  geom_point(shape = 21, color = "darkblue", fill = "white", size = 3)

```

Shapes for points can be specified using numbers; note how some shapes are "solid", i.e. specifying `color` will define the color of the shape. Others, like the one used in our example are filled, i.e. `color` will determine the color of the outline, and `fill` the color of the space inside. For "hollow" shapes (or e.g. X's) `color` will determine the line color.

![Numeric codes for sympols.](figures/points-symbols.png)

You can specify colors either using the [color names](http://applied-r.com/r-color-tables/) defined by R or using hex codes. 

```{marginfigure}

Some exploring will also lead you to various color pallets, Wes Anderson fan? You can [style your plots](https://github.com/karthik/wesanderson) accordingly... dig the aesthetics of the old school National Park posters and images? [R community got you covered.](https://github.com/katiejolly/nationalparkcolors).

```

Previously, we also used `geom_smooth()` to add a layer with a linear regression. If you take a look at the arguments for this function using `?geom_smooth` you will see that this has additional arguments apart from the mapping aesthetics like choosing the type of regression, whether or not the confidence interval is shown etc. Let's say we want to add a red, dashed regression line without the confidence interval.

```{r fig.cap="Line plot showing global mean temperature anomalies relative to 1951-1980 average. Individual data points added using geom_point(), color and fill for lines and shapes have been modified. Trend line fitted using geom_smooth()."}

ggplot(data = temperature, aes(x = Year, y = `J-D`)) +
  geom_line(color = "darkblue", size = 1) +
  geom_point(shape = 21, color = "darkblue", fill = "white", size = 3) +
  geom_smooth(stat = "smooth", se = FALSE, color = "red", linetype = "dashed", size = 2)

```

Admittedly, this plot is starting to look a little bit ridiculous. Maybe we should think of alternative options for visualizing this data to explore some additional `ggplot` options.

Another way to visualize this data would be using a bar plot^[Can we appreciate for a second how easy it was to change how the same data is plotted? If you were using excel you would have had to insert a new plot, define what data you wanted to plot, relabel the axis, etc. Here all you had to do was change a single line of code].

```{r fig.cap="Bar plot showing global mean temperature anomalies relative to 1951-1980 average."}

ggplot(data = temperature, aes(x = Year, y = `J-D`)) +
  geom_bar(stat = "identity")

```

Notice, how this is still the exact same mapping aesthetics (time/year on the x-axis and the temperature anomaly on the y-axis) but it changes the way we see the data completely - the emphasis now is on the change of temperatures below the long-term mean and the change to those temperature differences to being positive.

Let's say we want really lean into demonstrating this drastic change in temperatures not only being above the long-term average but also steadily increasing. One way to do this is to use color. We could modify this bar plot to have all of our bars representing years with global mean temperatures below the 1951-1980 average in one color and those above in another. 

To do this, we need a column that encodes that information... that of course is not an issue for us a basically professional data wranglers! We can use a simple conditional mutate using an `ifelse()` statement to add a column (`year_type`) that indicates whether temperatures are above or below the the long-term average.

We can manipulate our `data.frame` using the `dplyr` functions we are already familiar with - we can then use the `%>%` pipe to pass the data argument to the `ggplot()` function^[When you do this, remember that you need to switch back to using the `+` operator once you are adding your geom layers.]

```{r fig.cap="Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set."}

temperature %>%
  mutate(year_type = ifelse(`J-D` <=0, "colder", "hotter")) %>%
  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +
  geom_bar(stat = "identity")

```

Anything that we add to the `ggplot()` layer as a mapping is a universal plot setting that will apply to all subsequent layers. You can override these global settings is you specify aesthetics like color, shape or size specifically in an separate layer. For example if we do the following, we will override our color coding even though we specified it in our `ggplot()` layer.

```{r fig.cap="Bar plot showing global mean temperature anomalies relative to 1951-1980 average. Mapping aesthetics have been specified to color code according to variable included in the data set. Settings in geom_bar() override the general plot specifications."}

temperature %>%
  mutate(year_type = ifelse(`J-D` <=0, "colder", "hotter")) %>%
  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +
  geom_bar(stat = "identity", color = "blue", fill = "blue")

```

Let's see what else we can visualize using `ggplot`. Recall from when you explored our temperature anomaly data set, that this data set also contains information for seasons and individual months.

Let's take a look at the distribution of mean global winter temperatures (`DJF` = December, January, February) using a histogram, (`geom_histogram()`). Note that we only need to define our x-axis to plot a histogram, this is because `ggplot2` will count how many observations fall into each bin for you ... this geom is a mixture of a geometry (bar plot) and a statistical transformation (binning data points into ranges and the plotting those as bar plots).].

```{r fig.cap="Histogram displaying distribution of mean global winter temperatures."}

ggplot(temperature, aes(x = DJF)) +
  geom_histogram()

```

We can refine our plot by choosing our own binwidth and by manipulating fill and color.

```{r fig.cap="Histogram displaying distribution of mean global winter temperatures with customized bin width and coloration."}

ggplot(temperature, aes(x = SON)) +
  geom_histogram(binwidth = 0.05, color = "black", fill = "darkorange")

```

For our example we might also want to add a vertical line (`geom_vline()`) to indicate 0 (i.e. the long-term global temperature mean calculated for 1951 - 1980).

```{r fig.cap="Histogram displaying distribution of mean global winter temperatures with customized binwidth and coloration and added vertical line (red)."}


ggplot(temperature, aes(x = SON)) +
  geom_histogram(binwidth = 0.05, color = "black", fill = "darkorange") +
  geom_vline(xintercept = 0, color = "darkred", linetype = "dashed", size = 1)


```

If we want to compare multiple distributions, box plots can be more helpful than histograms. Let's say we wanted to compare the distribution of mean global temperature for each season.

How can we go about this?

Currently, our seasons are in individual columns, so our first step would be to create a tidy data set.

```{r}

tidy_season <- temperature %>%
  select(Year, DJF, MAM, JJA, SON) %>%
  pivot_longer(names_to = "season", values_to = "temperature", 2:5)

```

Now we can plot our seasons on the x-axis and the distribution of temperatures on the y-axis using `geom_boxplot()`.

```{r fig.cap="Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons."}

ggplot(tidy_season, aes(x = season, y = temperature)) +
  geom_boxplot()

```

Again, you can add additional information using the mapping aesthetics to e.g. color code the boxes by season.

```{r fig.cap="Box plot comparing global mean temperature anomalies relative to 1951-1980 average for all four seasons color-coded by season."}

ggplot(tidy_season, aes(x = season, y = temperature, fill = season)) +
  geom_boxplot()

```


## Faceting plots

One of the advantages of `ggplot2` being based not only on the grammar of graphics but being designed to play extremely well with tidy data sets is being able to create faceted plots. A faceted plot involves splitting a single plot into a matrix of panels, where each panel shows a different subset of the data. This is especially helpful during exploratory analysis where you might first plot all your data points in a single graph but then want to look at whether or not individual subsets within the data set behave the same.

Let's look at an example to better understand what faceting plots looks like. For example, let's say we wanted to create individual plots of our bar plots showing our deviations of global temperatures from the 1951 - 1980 mean for each season.

How could you create individual plots with the methods you are already familiar with?

Here is how you can do it using `facet_grid()`.

Now we can plot our data and using `facet_grid()` we can specify that we want individual panels by month in separate rows.

```{r fig.cap="Change in temperature anomaly relative to long-term avarage per season (rows)", fig.height=9}

ggplot(tidy_season, aes(x = Year, y = temperature)) +
  geom_bar(stat = "identity", fill = "darkorange", color = "darkorange") +
  facet_grid(rows = vars(season))

```

You could also specify that you want the individual plots to be separated into columns.

```{r fig.cap="Change in temperature anomaly relative to long-term avarage per season (columns)"}

ggplot(tidy_season, aes(x = Year, y = temperature)) +
  geom_bar(stat = "identity", fill = "darkorange", color = "darkorange") +
  facet_grid(cols = vars(season))

```

You can also create faceted plot where subset your data by two variables and so you end up with one variable defining the rows and one the columns and you can use a simpler syntax `row-variable ~ column-variable`.

You can also use this syntax if you are only faceting by one variable as we are doing in this example by specifying the variable and leaving the other one "blank" using a `.`.

For example, to plot this faceted data set in rows, you would use the following syntax - 

```{r fig.cap="Change in temperature anomaly relative to long-term avarage per season (rows)", fig.height=9}


ggplot(tidy_season, aes(x = Year, y = temperature)) +
  geom_bar(stat = "identity", fill = "darkorange", color = "darkorange") +
  facet_grid(season ~ .)


```

And to plot this data set column-wise you would specify it like so -

```{r fig.cap="Change in temperature anomaly relative to long-term avarage per season (columns)"}


ggplot(tidy_season, aes(x = Year, y = temperature)) +
  geom_bar(stat = "identity", fill = "darkorange", color = "darkorange") +
  facet_grid(. ~ season)


```


## Customizing plots

We have already played around a little bit with the options that we have in terms of customizing plots using color, fill, shapes, and sizes. But we've barely scratched the surface.

You have probably noticed that there are some default settings like the gray background and white grid lines, font sizes of label axis, what the axis labels are, and even color schemes that are automatically used. Even using the defaults we get pretty clean plots that are visually appealing. This is super helpful during exploratory analysis because even though you playing around with the data you still have nicely formatted and easy to interpret figures.

Once you have identified the central plots that you want to use to communicate the results and conclusions of your data analysis you will want to further customize your visualization to optimize communication, this includes how you encode data using color and shape but also making sure that everything is well labeled and clear to the person who is reading your report or listening to your presentation.

One of the first things we frequently want to changes is the axis labels. `ggplot2` does handily use the column names to automatically label your axes, so you will always have a label which is great during exploratory analysis but generally you will want to customize that for your final figure. The function `labs()` can be used to specify a title, subtitle, axis labels and additional annotations (caption) below the figure.

For example we could customize our faceted figure to look like this:

```{r fig.cap="Change in temperature anomaly relative to long-term avarage per season (rows)", fig.height=9}

ggplot(tidy_season, aes(x = Year, y = temperature)) +
  geom_bar(stat = "identity", fill = "darkorange", color = "darkorange") +
  facet_grid(rows = vars(season)) +
  labs(title = "Change in global seasonal temperatures 1880 - 2020",
       subtitle = "Global mean temperatures relative to 1951 - 1980 mean",
       x = "Year", y = "Temperature [C]",
       caption = "Date source: NASA Goddard Institute for Space Studies")

```

Pretty much every component of a `ggplot` figure can be further customized using `theme()`, this includes things like font size, background and line colors, grids, legend position ... `ggplot2` odes have some pre-defined themes that you can call up that will change the layout. 

The default theme is `theme_grey()`.

```{r fig.cap="Change in temperature anomaly relative to long-term avarage per season (rows)", fig.height=9}

ggplot(tidy_season, aes(x = Year, y = temperature)) +
  geom_bar(stat = "identity", fill = "darkorange", color = "darkorange") +
  facet_grid(rows = vars(season)) +
  labs(title = "Change in global seasonal temperatures 1880 - 2020",
       subtitle = "Global mean temperatures relative to 1951 - 1980 mean",
       x = "Year", y = "Temperature [C]",
       caption = "Date source: NASA Goddard Institute for Space Studies") +
  theme_grey()

```

Here, we explicitly specified it using `theme_grey()`; if you do not specify a theme this is the theme that will be used for your plot. There are other themes that are part of the `ggplot` package that include `theme_bw()`, `theme_minimal()`, `theme_classic()` or `theme_light()`.

`r msmbstyle::question_begin(label = "ques:Vis-1")`

Re-plot the same figure using the four themese specified above.

`r msmbstyle::question_end()`


You will likely find a theme that you like but still want to make additional tweaks in that case `theme()` is your friend. 

Let's go back to our bar plot of the global temperature anomaly to look at an example of likely the three most common things you will want to adjust which is the legend position, changing font size, color, turning x-axis labels by 90 degrees.

```{r fig.cap="Change in mean global temperature relative to long-term avaerage with customized font sizes and legend positions."}

temperature %>%
  mutate(year_type = ifelse(`J-D` <=0, "colder", "hotter")) %>%
  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Change in global mean temperatures 1880 - 2020",
       subtitle = "Global mean temperatures relative to 1951 - 1980 mean",
       x = "Year", y = "Temperature [C]",
       caption = "Date source: NASA Goddard Institute for Space Studies") +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(size = 12, color = "black",
                                   angle = 90, hjust = 0.5, vjust = 0.5),
        axis.text.y = element_text(size = 12, color = "black",),
        axis.title = element_text(size = 14))

```

That's starting to look pretty slick.

As you start your visualization adventure the [R Cookbook](https://www.cookbook-r.com/Graphs/Colors_(ggplot2)/) is a good resource. It is written in a helpful style that starts with a concrete plotting problem and then walks you through a solution.


## Arranging plots

We've already seen that we can create multi-panel plots in a very straightforward way when we are essentially plotting the same plot for different subsets of the data.

But what if we want to generate individual plots that might fit together thematically but aren't subsets that we can facet but we still want to be able to present them together? Fear not, this too can be solved; one option is using `patchwork`, a package designed for exactly this purpose. Let's generate a few additional plots so we can try this out.

In our data folder there is a tab-delimited file with monthly mean CO2 concentrations (parts per million) from Maunua Loa. Let's read in that data set and create line plot with a regression line.

```{r fig.cap="Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory from 1958 - 2021."}

carbon <- read_delim("data/CO2_monthly.txt", delim = "\t")

ggplot(carbon, aes(x = date, y = average)) +
  geom_line(color = "blue") +
  geom_smooth(color = "red") +
  labs(title = "Atmospheric CO2",
       subtitle = "monthly mean CO2 Mauna Loa CO2",
       x = "year", y = "CO2 concentration air [ppm]",
       caption = "Data: NOAA/ESRL") +
  theme_classic()

```

We also have a data set from the Global Carbon Project downloaded from the [Our World in Data repository](https://github.com/owid/co2-data) that contains atmospheric CO2 emissions. We can plot that as a simple line plot with a regression.

```{r fig.cap="Global atmospheric CO2 emissions."}

emissions <- read_delim("data/emissions.txt", delim = "\t") %>%
  filter(iso_code == "OWID_WRL")

ggplot(emissions, aes(x = year, y = co2)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(color = "red") +
  labs(title = "CO2 Emissions over time.",
       subtitle = "Global emissions atmospheric emissions by year.",
       x = "year", y = "CO2 [Gt/year]",
       caption = "Data: Global Carbon Project/Our World in Data") +
  theme_classic()

```

To be able to plot multiple plots in one using `patchwork` we need to assign our figures as objects.

```{r}

p1 <- ggplot(carbon, aes(x = date, y = average)) +
  geom_line(color = "blue") +
  geom_smooth(color = "red") +
  labs(title = "Atmospheric CO2",
       subtitle = "monthly mean CO2 Mauna Loa CO2",
       x = "year", y = "CO2 concentration air [ppm]",
       caption = "Data: NOAA/ESRL") +
  theme_classic()

p2 <- ggplot(emissions, aes(x = year, y = co2)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(color = "red") +
  labs(title = "Atmospheric CO2 Emissions",
       subtitle = "global emissions",
       x = "year", y = "CO2 [Gt/year]",
       caption = "Data: Global Carbon Project/Our World in Data") +
  theme_classic()

```

Now we can combine them side by side using a simple syntax.

```{r fig.cap="Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (left panel) and Global atmospheric CO2 emissions (right panel)."}

p1 + p2

```

Similarly, we can plot them underneath each other like so -

```{r fig.cap="Mean monthly atmospheric CO2 concentrations measuread at Mauna Loa Observatory (top panel) and Global atmospheric CO2 emissions (bottom panel).", fig.height=7}

p1 / p2

```

Oh, it gets better. Let's say we wanted to plot our global mean temperatures in the top row and our two emissions plots below.

```{r fig.cap="Comparison of change in mean global temperature (top panel), atmospheric CO2 concentrations (bottom left), and atmospheric CO2 emissions (bottom right).", fig.height=11}

p3 <- temperature %>%
  mutate(year_type = ifelse(`J-D` <=0, "colder", "hotter")) %>%
  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Change in global mean temperatures 1880 - 2020",
       subtitle = "Global mean temperatures relative to 1951 - 1980 mean",
       x = "Year", y = "Temperature [C]",
       caption = "Date source: NASA Goddard Institute for Space Studies") +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(size = 12, color = "black",
                                   angle = 90, hjust = 0.5, vjust = 0.5),
        axis.text.y = element_text(size = 12, color = "black",),
        axis.title = element_text(size = 14))

p3 / (p1 | p2)

```

You can create complex compositions in `patchwork` using syntax combining `+`, `|`, and `\`. To see how you can control the layout even further you can check out the [documentation for patchwork](https://patchwork.data-imaginist.com/articles/guides/layout.html).


## Exporting Plots

The last thing we still have to figure out is how to save plots. You have a few options.

The quickest and dirtiest option is to simply right click on the plot pane after plotting a figure and then save the figure using `Save image as`. This works and is easy to do but gives you very little control over the dimensions, resolution, file format etc.

For more control over the format of your figure you can use the Export tab in the Plot pane which will allow you to adjust the dimensions and the the file format.

Finally, the `ggplot2` has a function called `ggsave()` that will allow you to determine the dimensions (`width`, `height`), resolution (`dpi`), and format (`device`).

By default it will save the last plot that was plotted. If you specify the format in the file name (e.g. `*.svg`, `*.jpg`, `*.png`) it will automatically recognize the format^[Remember that we have designed our research compendium to keep raw and processed data, and results separate? Figures are considered results so you should always save them to the `results` folder.].

```{r fig.cap="Change in mean global temperature realtive to long-term average."}

temperature %>%
  mutate(year_type = ifelse(`J-D` <=0, "colder", "hotter")) %>%
  ggplot(aes(x = Year, y = `J-D`, fill = year_type, color = year_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Change in global mean temperatures 1880 - 2020",
       subtitle = "Global mean temperatures relative to 1951 - 1980 mean",
       x = "Year", y = "Temperature [C]",
       caption = "Date source: NASA Goddard Institute for Space Studies") +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(size = 12, color = "black",
                                   angle = 90, hjust = 0.5, vjust = 0.5),
        axis.text.y = element_text(size = 12, color = "black",),
        axis.title = element_text(size = 14))

ggsave("results/global_temp.png", dpi = 300, width = 15, height = 10)

```

Check your results folder to see if you were successful!

If you assign your plot to an object you can use the `plot` argument of the `ggsave()` function to export it. This also works for figures that consist of multiple panels combined using `patchwork`.

## More Plots!

Ready to take the training wheels off? 

Increasing global temperatures are effecting change across the different components of the climate system. There are several data sets in your `data` folder. Read in each data set as a data frame, use `View()` to take a look at what information is contained in the data set and the use your new found visualization skills to plot the data using `ggplot`.

`r msmbstyle::question_begin(label = "ques:Vis-2")`

Increasing atmospheric temperatures result in land ice melting and the ocean heat content rising. The latter leads to thermal expansion which together with the increase freshwater inflow results in rising sea levels.

The first data set is from the NOAA Laboratory for Satellite Altimetry and contains records from coastal sea level tide gauges. Start by reading in the data set

```{r}

sealevel <- read_delim("data/sealevel.txt", delim = "\t")

```

Take a quick look at the data set using `Vew()` and briefly summarize what data is contained in this data set^[Include things like the number of columns, what data is contained in each sample, what time periods are included etc. Practice deducing these types of things from the context of the data set description above and by browsing the content.]

Plot the change in means sea level over time as a line plot and color code the line(s) by method. Chose a theme and position your legend below the figure. Bonus: Figure out how to use `geom_hline()` to add a line at 0.

Comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.

Write a short description of your figure summarizing the key results displayed.

Here's an example of what that could look like.

```{r fig.cap="Change in mean global coastal sea level using costal sea level tide gauges (blue) and satellite data (red).", echo=FALSE}

ggplot(sealevel, aes(x = date, y = gmsl, color = method)) +
  geom_line() +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  scale_color_manual(values = c("red", "darkblue")) +
  labs(title = "Change in mean global coastal sea level",
       subtitle = "Coastal sea level tide gauges and satellite data (1880 - 2021)",
       caption = "NOAA Laboratory for Satellite Altimetry",
       x = "date", y = "change in mean sea level [mm]") +
  theme_standard +
  theme(legend.position = "bottom")

```

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:Vis-3")`

Our next data set is from the [National Snow and Ice Data Center](https://nsidc.org/data/seaice_index/archives) and contains the Arcitc (Northern Hemisphere) July sea ice extent.

```{r}

ice_arctic <- read_delim("data/arctic_ice.txt", delim = "\t")

```

Take a quick look at the data set using `Vew()` and briefly summarize what data is contained in this data set. Be specific.

Plot the change in ice extent over time as a line plot. Chose a theme. Add a linear regression and confidence interval. Bonus: Change the default colors of the regression line and the confidence interval.

Comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.

Write a short description of your figure summarizing the key results displayed.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:Vis-4")`

Let's take a look at the equivalent data set depicting sea ice extent in the Antarctic (Southern Hemisphere.)

```{r}

ice_arctic <- read_delim("data/antarctic_ice.txt", delim = "\t")

```

Take a quick look at the data set using `Vew()` and briefly summarize what data is contained in this data set. Be specific.

Plot the change in ice extent over time as a line plot. Chose a theme. Add a linear regression and confidence interval. Bonus: Change the default colors of the regression line and the confidence interval.

Comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.

Write a short description of your figure summarizing the key results displayed.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin(label = "ques:Vis-5")`

One more data set. This one is from the NOAA Hurricane Research Division. While interpreting the observed patterns in sea level and ice extent and modeling future projections is pretty straightforward. For this coding challenge you will need to combine your data wrangling and visualization skills!

```{r}

hurricane <- read_delim("data/hurricanes.txt", delim = "\t")

```

Take a quick look at the data set using `Vew()` and briefly summarize what data is contained in this data set. Be specific^[Note, several of the columns start with 'Revised' this just has to do with the entire data set having revised (most recent) definitions applied for the metrics included. You can ignore that component.].

When thinking about hurricanes there are two metrics to consider when determining whether or not "hurricanes have gotten worse". First we can look at whether or not hurricanes have become more common by plotting the number of hurricanes for each year. Create a faceted plot with individual line plots showing the number of named storms (this would include not only hurricanes put also tropical storms), the number of hurricanes, the number of major hurricanes, and the number of hurricanes in the United States. Add a linear regression. Chose a custom theme.

You will have to reformat the data set in order to plot it in this fashion. It can be helpful to first think about what your `ggplot` code should look like to create the plot described above and then format your data set to fit those needs^[Recall that ggplot is designed not only around the grammar of graphics but also the principles of tidy data... do we currently have a tidy data set?]. 

Bonus: We've previously used `facet_grid()` to create faceted plots which requires us to specify how panels should be laid out in rows and/or columns. Figure out how to use `facet_wrap()` to just specify which variable to use to create individual panels with out them all needing to be laid out next to each other in columns or underneath each other in rows. 

```{r eval=FALSE, include=FALSE}

hurricane %>%
  select(-RevisedACE) %>%
  pivot_longer(names_to = "category", values_to = "frequency", cols = 2:5) %>%
  ggplot(aes(x = Year, y = frequency)) +
    geom_line() +
    geom_smooth(method = "lm") +
    facet_wrap(~ category)

```

The other way to evaluate whether or not "hurricanes have gotten worse" is to not look at the number of storms but to determine whether individual storms or storms as a whole have become more intense and/or destructive.

The one column you probably have not yet been able to figure out is the column containing information on the "accumulated cyclone energy"^[Now that you have this information if you have not already add it to your short data set description.].

Create a bar plot showing the ACE for each year. Chose a theme and customize the color of your bars^[You can specify color and fill for bar plots. Try setting the color and fill to the same and to different colors to see what happens when you have a bar plot with this many individual bars.] 

Bonus: Tru to figure out how to use `geom_hline()` to add a horizontal line indicating the mean ACE for the recorded time period.

```{r plot-14, eval=FALSE, include=FALSE}

ggplot(hurricane, aes(x = Year, y = RevisedACE)) +
  geom_bar(stat = "identity", color = "darkorange", fill = "darkorange") +
  geom_hline(aes(yintercept = mean(RevisedACE)))

```

For all the plots, comment each line of your ggplot code (you can add comments between the lines or at the end), remember to caption your figure, list your data source and and label your axis in a meaningful way.

Write a short description for each figure summarizing the key results displayed and argue whether or not you thing "hurricanes have gotten worse".

`r msmbstyle::question_end()`

`r msmbstyle::question_begin(label = "ques:Vis-6")`

Choose five of the plots we have generated in this module and combine them into a single plot using `patchwork`; include at least two columns and two rows in your layout. 

Bonus: You can use `fig.height=` and `fig.width=` in the first line starting a new code to specify the dimensions of the output of a figure in the knitted document. When you initially knit your Rmarkdown file you will find that the default dimensions will not format your you multi-panel plot in a way that does not make it super legible. Manipulate these parameters and re-knit your html to display your figure in an appropriate way.

`r msmbstyle::question_end()`

<!--chapter:end:14_IntroVisualization.Rmd-->


`r if (knitr::is_html_output()) '# References {-}'`

<!--chapter:end:99_References.Rmd-->

